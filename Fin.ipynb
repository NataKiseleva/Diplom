{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fin.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN22srU+YcNwyULy+L/koEp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataKiseleva/Diplom/blob/main/Fin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install read"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A_zOIu94Auc",
        "outputId": "9d38238e-099b-4f2f-980d-9228fb4c58ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: read in /usr/local/lib/python3.7/dist-packages (0.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWygSqaA3tHm"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import inspect\n",
        "import bisect\n",
        "from   itertools import chain\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "# import ujson as json\n",
        "\n",
        "import keras.layers as kl\n",
        "import keras.backend as kb\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "#from read import extract_morpheme_types, read_BMES, read_splitted\n",
        "#from tabled_trie import make_trie  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def generate_BMES(morphs, morph_types):\n",
        "    answer = []\n",
        "    for morph, morph_type in zip(morphs, morph_types):\n",
        "        if len(morph) == 1:\n",
        "            answer.append(\"S-\" + morph_type)\n",
        "        else:\n",
        "            answer.append(\"B-\" + morph_type)\n",
        "            answer.extend([\"M-\" + morph_type] * (len(morph) - 2))\n",
        "            answer.append(\"E-\" + morph_type)\n",
        "    return answer\n",
        "\n",
        "\n",
        "def read_splitted(infile, transform_to_BMES=True, n=None, morph_sep=\"/\", shuffle=True):\n",
        "    source, targets = [], []\n",
        "    with open(infile, \"r\", encoding=\"utf8\") as fin:\n",
        "        for line in fin:\n",
        "            line = line.strip()\n",
        "            if line == \"\":\n",
        "                break\n",
        "            word, analysis = line.split(\"\\t\")\n",
        "            morphs = analysis.split(morph_sep)\n",
        "            morph_types = [\"None\"] * len(morphs)\n",
        "            if transform_to_BMES:\n",
        "                target = generate_BMES(morphs, morph_types)\n",
        "            else:\n",
        "                target = morph_types\n",
        "            source.append(word)\n",
        "            targets.append(target)\n",
        "    indexes = list(range(len(source)))\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indexes)\n",
        "    if n is not None:\n",
        "        indexes = indexes[:n]\n",
        "    source = [source[i] for i in indexes]\n",
        "    targets = [targets[i] for i in indexes]\n",
        "    return source, targets\n",
        "\n",
        "\n",
        "def read_BMES(infile, transform_to_BMES=True, n=None,\n",
        "              morph_sep=\"/\" ,sep=\":\", shuffle=True):\n",
        "    source, targets = [], []\n",
        "    with open(infile, \"r\", encoding=\"utf8\") as fin:\n",
        "        for line in fin:\n",
        "            line = line.strip()\n",
        "            if line == \"\":\n",
        "                break\n",
        "            word, analysis = line.split(\"\\t\")\n",
        "            analysis = [x.split(sep) for x in analysis.split(morph_sep)]\n",
        "            morphs, morph_types = [elem[0] for elem in analysis], [elem[1] for elem in analysis]\n",
        "            target = generate_BMES(morphs, morph_types) if transform_to_BMES else morphs\n",
        "            source.append(word)\n",
        "            targets.append(target)\n",
        "    indexes = list(range(len(source)))\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indexes)\n",
        "    if n is not None:\n",
        "        indexes = indexes[:n]\n",
        "    source = [source[i] for i in indexes]\n",
        "    targets = [targets[i] for i in indexes]\n",
        "    return source, targets\n",
        "\n",
        "\n",
        "def partition_to_BMES(s1, s2):\n",
        "    morphemes = s1.split(\"/\")\n",
        "    labels = s2.split(\" , \")\n",
        "    answer = []\n",
        "    for l, m in zip(labels, morphemes):\n",
        "        length = len(m)\n",
        "        if l.startswith(\"Корень\"):\n",
        "            if m.startswith(\"-\"):\n",
        "                    answer.append(\"S-HYPH\")\n",
        "                    length -= 1\n",
        "            if length == 1:\n",
        "                answer.append(\"S-ROOT\")\n",
        "            else:\n",
        "                answer.append(\"B-ROOT\")\n",
        "                for i in range(length-2):\n",
        "                    answer.append(\"M-ROOT\")\n",
        "                answer.append(\"E-ROOT\")\n",
        "\n",
        "        elif l.startswith(\"Приставка\"):\n",
        "            if m.startswith(\"-\"):\n",
        "                    answer.append(\"S-HYPH\")\n",
        "                    length -= 1\n",
        "            if length == 1:\n",
        "                answer.append(\"S-PREF\")\n",
        "            else:\n",
        "                answer.append(\"B-PREF\")\n",
        "                for i in range(length-2):\n",
        "                    answer.append(\"M-PREF\")\n",
        "                answer.append(\"E-PREF\")\n",
        "\n",
        "        elif l.startswith(\"Суффикс\"):\n",
        "            if length == 1:\n",
        "                answer.append(\"S-SUFF\")\n",
        "            else:\n",
        "                answer.append(\"B-SUFF\")\n",
        "                for i in range(length-2):\n",
        "                    answer.append(\"M-SUFF\")\n",
        "                answer.append(\"E-SUFF\")\n",
        "\n",
        "        elif l.startswith(\"Соединительная гласная\") is True:\n",
        "            answer.append(\"S-LINK\")\n",
        "\n",
        "        elif l.startswith(\"Окончание\") is True:\n",
        "            if length == 1:\n",
        "                answer.append(\"S-END\")\n",
        "            else:\n",
        "                answer.append(\"B-END\")\n",
        "                for i in range(length-2):\n",
        "                    answer.append(\"M-END\")\n",
        "                answer.append(\"E-END\")\n",
        "\n",
        "        #elif l.startswith(\"Нулевое окончание\") is True:\n",
        "            #answer.append(\"S-NULL_END\")\n",
        "\n",
        "        elif l.startswith(\"Постфикс\") is True:\n",
        "            if m.startswith(\"-\") is True:\n",
        "                answer.append(\"HYPH\")\n",
        "                length -= 1\n",
        "            answer.append(\"B-POSTFIX\")\n",
        "            for i in range(length-2):\n",
        "                answer.append(\"M-POSTFIX\")\n",
        "            answer.append(\"E-POSTFIX\")\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "def extract_morpheme_type(x):\n",
        "    return x[2:].lower()\n",
        "\n",
        "\n",
        "def read_input(infile, transform_to_BMES=True, n=None, shuffle=True):\n",
        "    source, targets = [], []\n",
        "    with open(infile, \"r\", encoding=\"utf8\") as fin:\n",
        "        for line in fin:\n",
        "            line = line.strip()\n",
        "            if line == \"\":\n",
        "                break\n",
        "            word, morphs, analysis = line.split(\" | \")\n",
        "            target = partition_to_BMES(morphs, analysis) if transform_to_BMES else morphs\n",
        "            source.append(word)\n",
        "            targets.append(target)\n",
        "    if n is not None:\n",
        "        indexes = list(range(len(source)))\n",
        "        if shuffle:\n",
        "            np.random.shuffle(indexes)\n",
        "        indexes = indexes[:n]\n",
        "        source = [source[i] for i in indexes]\n",
        "        targets = [targets[i] for i in indexes]\n",
        "    return source, targets"
      ],
      "metadata": {
        "id": "JzOJImosZTGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrieMinimizer:\n",
        "    '''\n",
        "    Класс для сжатия префиксного бора\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def minimize(self, trie, dict_storage=False, make_cashed=False, make_numpied=False,\n",
        "                 precompute_symbols=None, allow_spaces=False, return_groups=False):\n",
        "        N = len(trie)\n",
        "        if N == 0:\n",
        "            raise ValueError(\"Trie should be non-empty\")\n",
        "        node_classes = np.full(shape=(N,), fill_value=-1, dtype=int)\n",
        "        order = self.generate_postorder(trie)\n",
        "        # processing the first node\n",
        "        index = order[0]\n",
        "        node_classes[index] = 0\n",
        "        class_representatives = [index]\n",
        "        node_key = ((), (), trie.is_final(index))\n",
        "        classes, class_keys = {node_key : 0}, [node_key]\n",
        "        curr_index = 1\n",
        "        for index in order[1:]:\n",
        "            letter_indexes = tuple(trie._get_letters(index, return_indexes=True))\n",
        "            children = trie._get_children(index)\n",
        "            children_classes = tuple(node_classes[i] for i in children)\n",
        "            key = (letter_indexes, children_classes, trie.is_final(index))\n",
        "            key_class = classes.get(key, None)\n",
        "            if key_class is not None:\n",
        "                node_classes[index] = key_class\n",
        "            else:\n",
        "                # появился новый класс\n",
        "                class_keys.append(key)\n",
        "                classes[key] = node_classes[index] = curr_index\n",
        "                class_representatives.append(curr_index)\n",
        "                curr_index += 1\n",
        "        # построение нового дерева\n",
        "        compressed = Trie(trie.alphabet, is_numpied=make_numpied,\n",
        "                          dict_storage=dict_storage, allow_spaces=allow_spaces,\n",
        "                          precompute_symbols=precompute_symbols)\n",
        "        L = len(classes)\n",
        "        new_final = [elem[2] for elem in class_keys[::-1]]\n",
        "        if dict_storage:\n",
        "            new_graph = [defaultdict(int) for _ in range(L)]\n",
        "        elif make_numpied:\n",
        "            new_graph = np.full(shape=(L, len(trie.alphabet)),\n",
        "                                fill_value=Trie.NO_NODE, dtype=int)\n",
        "            new_final = np.array(new_final, dtype=bool)\n",
        "        else:\n",
        "            new_graph = [[Trie.NO_NODE for a in trie.alphabet] for i in range(L)]\n",
        "        for (indexes, children, final), class_index in\\\n",
        "                sorted(classes.items(), key=(lambda x: x[1])):\n",
        "            row = new_graph[L-class_index-1]\n",
        "            for i, child_index in zip(indexes, children):\n",
        "                row[i] = L - child_index - 1\n",
        "        compressed.graph = new_graph\n",
        "        compressed.root = L - node_classes[trie.root] - 1\n",
        "        compressed.final = new_final\n",
        "        compressed.nodes_number = L\n",
        "        compressed.data = [None] * L\n",
        "        if make_cashed:\n",
        "            compressed.make_cashed()\n",
        "        if precompute_symbols is not None:\n",
        "            if (trie.is_terminated and trie.precompute_symbols\n",
        "                    and trie.allow_spaces == allow_spaces):\n",
        "                # копируем будущие символы из исходного дерева\n",
        "                # нужно, чтобы возврат из финальных состояний в начальное был одинаковым в обоих деревьях\n",
        "                for i, node_index in enumerate(class_representatives[::-1]):\n",
        "                    # будущие символы для представителя i-го класса\n",
        "                    compressed.data[i] = copy.copy(trie.data[node_index])\n",
        "            else:\n",
        "                precompute_future_symbols(compressed, precompute_symbols, allow_spaces)\n",
        "        if return_groups:\n",
        "            node_classes = [L - i - 1 for i in node_classes]\n",
        "            return compressed, node_classes\n",
        "        else:\n",
        "            return compressed\n",
        "\n",
        "    def generate_postorder(self, trie):\n",
        "        '''\n",
        "        Обратная топологическая сортировка\n",
        "        '''\n",
        "        order, stack = [], []\n",
        "        stack.append(trie.root)\n",
        "        colors = ['white'] * len(trie)\n",
        "        while len(stack) > 0:\n",
        "            index = stack[-1]\n",
        "            color = colors[index]\n",
        "            if color == 'white': # вершина ещё не обрабатывалась\n",
        "                colors[index] = 'grey'\n",
        "                for child in trie._get_children(index):\n",
        "                    # проверяем, посещали ли мы ребёнка раньше\n",
        "                    if child != Trie.NO_NODE and colors[child] == 'white':\n",
        "                        stack.append(child)\n",
        "            else:\n",
        "                if color == 'grey':\n",
        "                    colors[index] = 'black'\n",
        "                    order.append(index)\n",
        "                stack = stack[:-1]\n",
        "        return order\n",
        "\n",
        "def load_trie(infile):\n",
        "    with open(infile, \"r\", encoding=\"utf8\") as fin:\n",
        "        line = fin.readline().strip()\n",
        "        flags = [x=='T' for x in line.split()]\n",
        "        if len(flags) != len(Trie.ATTRS) + 1:\n",
        "            raise ValueError(\"Wrong file format\")\n",
        "        nodes_number, root = map(int, fin.readline().strip().split())\n",
        "        alphabet = fin.readline().strip().split()\n",
        "        trie = Trie(alphabet)\n",
        "        for i, attr in enumerate(Trie.ATTRS):\n",
        "            setattr(trie, attr, flags[i])\n",
        "        read_data = flags[-1]\n",
        "        final = [False] * nodes_number\n",
        "        print(len(alphabet), nodes_number)\n",
        "        if trie.dict_storage:\n",
        "            graph = [defaultdict(lambda: -1) for _ in range(nodes_number)]\n",
        "        elif trie.is_numpied:\n",
        "            final = np.array(final)\n",
        "            graph = np.full(shape=(nodes_number, len(alphabet)),\n",
        "                            fill_value=Trie.NO_NODE, dtype=int)\n",
        "        else:\n",
        "            graph = [[Trie.NO_NODE for a in alphabet] for i in range(nodes_number)]\n",
        "        for i in range(nodes_number):\n",
        "            line = fin.readline().strip()\n",
        "            if \"\\t\" in line:\n",
        "                label, transitions = line.split(\"\\t\")\n",
        "                final[i] = (label == \"T\")\n",
        "            else:\n",
        "                label = line\n",
        "                final[i] = (label == \"T\")\n",
        "                continue\n",
        "            transitions = [x.split(\":\") for x in transitions.split()]\n",
        "            for code, value in transitions:\n",
        "                graph[i][int(code)] = int(value)\n",
        "        trie.graph = graph\n",
        "        trie.root = root\n",
        "        trie.final = final\n",
        "        trie.nodes_number = nodes_number\n",
        "        trie.data = [None] * nodes_number\n",
        "        if read_data:\n",
        "            for i in range(nodes_number):\n",
        "                line = fin.readline().strip(\"\\n\")\n",
        "                trie.data[i] = [set(elem.split(\",\")) for elem in line.split(\":\")]\n",
        "        if trie.to_make_cashed:\n",
        "            trie.make_cashed()\n",
        "        return trie\n",
        "\n",
        "\n",
        "def make_trie(words, alphabet=None, compressed=True, is_numpied=False,\n",
        "              make_cashed=False, precompute_symbols=False,\n",
        "              allow_spaces=False, dict_storage=False):\n",
        "    if alphabet is None:\n",
        "        alphabet = sorted({x for word in words for x in word})\n",
        "    trie = Trie(alphabet, is_numpied=is_numpied, to_make_cashed=make_cashed,\n",
        "                precompute_symbols=precompute_symbols, dict_storage=dict_storage)\n",
        "    trie.fit(words)\n",
        "    print(len(trie))\n",
        "    if compressed:\n",
        "        tm = TrieMinimizer()\n",
        "        trie = tm.minimize(trie, dict_storage=dict_storage, make_cashed=make_cashed,\n",
        "                           make_numpied=is_numpied, precompute_symbols=precompute_symbols,\n",
        "                           allow_spaces=allow_spaces)\n",
        "        print(len(trie))\n",
        "    return trie\n",
        "\n",
        "def precompute_future_symbols(trie, n, allow_spaces=False):\n",
        "    '''\n",
        "    Collecting possible continuations of length <= n for every node\n",
        "    '''\n",
        "    if n == 0:\n",
        "        return\n",
        "    if trie.is_terminated and trie.precompute_symbols:\n",
        "        # символы уже предпосчитаны\n",
        "        return\n",
        "    for index, final in enumerate(trie.final):\n",
        "        trie.data[index] = [set() for i in range(n)]\n",
        "    for index, (node_data, final) in enumerate(zip(trie.data, trie.final)):\n",
        "        node_data[0] = set(trie._get_letters(index))\n",
        "        if allow_spaces and final:\n",
        "            node_data[0].add(\" \")\n",
        "    for d in range(1, n):\n",
        "        for index, (node_data, final) in enumerate(zip(trie.data, trie.final)):\n",
        "            children = set(trie._get_children(index))\n",
        "            for child in children:\n",
        "                node_data[d] |= trie.data[child][d - 1]\n",
        "            # в случае, если разрешён возврат по пробелу в стартовое состояние\n",
        "            if allow_spaces and final:\n",
        "                node_data[d] |= trie.data[trie.root][d - 1]\n",
        "    trie.terminated = True\n",
        "\n",
        "def test_basic():\n",
        "    alphabet = \"abc\"\n",
        "    trie = Trie(alphabet, allow_spaces=True, dict_storage=True)\n",
        "    words = [\"aba\", \"acba\", \"b\", \"bab\", \"a\", \"cb\"]\n",
        "    trie.fit(words)\n",
        "    print(trie)\n",
        "    tm = TrieMinimizer()\n",
        "    compressed = tm.minimize(trie, make_numpied=False, precompute_symbols=2,\n",
        "                             make_cashed=True, allow_spaces=True)\n",
        "    print(compressed)\n",
        "    compressed.save(\"trie.in\")\n",
        "    compressed = load_trie(\"trie.in\")\n",
        "    print(compressed.find_partitions('acbacb', 3))\n",
        "    for word in compressed.words():\n",
        "        print(word)\n",
        "    # print(compressed.find_partitions('aba', 1))\n",
        "    # print(compressed.find_partitions('abab', 1))\n",
        "    # print(compressed.find_partitions('abab', 2))\n",
        "\n",
        "\n",
        "def test_performance():\n",
        "    alphabet = 'абвгдеёжзийклмнопрстуфхцчшщьыъэюя-'\n",
        "    infile = \"test_data/words_100000.txt\"\n",
        "    words = []\n",
        "    with open(infile, \"r\", encoding=\"utf8\") as fin:\n",
        "        for line in fin:\n",
        "            line = line.strip().lower()\n",
        "            if len(line) != 0:\n",
        "                words.append(line)\n",
        "    tm = TrieMinimizer()\n",
        "    # дерево на списках\n",
        "    trie = Trie(alphabet, is_numpied=False, precompute_symbols=2)\n",
        "    t1 = time.time()\n",
        "    trie.fit(words[:90000])\n",
        "    # trie.make_numpied()\n",
        "    t2 = time.time()\n",
        "    for word in words[10000:]:\n",
        "        flag = (word in trie)\n",
        "    t3 = time.time()\n",
        "    trie.save(\"trie.out\")\n",
        "    t4 = time.time()\n",
        "    trie = load_trie(\"trie.out\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    t5 = time.time()\n",
        "    print(\"{:.3f} {:.3f} {:.3f} {:.3f}\".format(t5 - t4, t4-t3, t3-t2, t2-t1))\n",
        "    compressed = tm.minimize(trie, make_numpied=False, make_cashed=True, precompute_symbols=2)\n",
        "    t6 = time.time()\n",
        "    for word in words[10000:]:\n",
        "        flag = (word in compressed)\n",
        "    t7 = time.time()\n",
        "    compressed.save(\"trie_compressed.out\")\n",
        "    t8 = time.time()\n",
        "    compressed = load_trie(\"trie_compressed.out\")\n",
        "    t9 = time.time()\n",
        "    print(\"{:.3f} {:.3f} {:.3f}\".format(t9-t8, t8-t7, t7-t6))\n",
        "\n",
        "def test_encoding():\n",
        "    alphabet = 'абвгдеёжзийклмнопрстуфхцчшщьыъэюя-'\n",
        "    infile = \"test_data/words_1000000.txt\"\n",
        "    words = []\n",
        "    with open(infile, \"r\", encoding=\"utf8\") as fin:\n",
        "        for line in fin:\n",
        "            line = line.strip().lower()\n",
        "            if len(line) != 0:\n",
        "                words.append(line)\n",
        "    tm = TrieMinimizer()\n",
        "    # дерево на списках\n",
        "    trie = Trie(alphabet, is_numpied=False)\n",
        "    t1 = time.time()\n",
        "    for word in words[:90000]:\n",
        "        trie.add(word)\n",
        "    trie.make_cashed()\n",
        "    # trie.make_numpied()\n",
        "    t2 = time.time()\n",
        "    for word in words[10000:]:\n",
        "        flag = (word in trie)\n",
        "    # минимизация\n",
        "    print(\"{:.3f} {:.3f}\".format(time.time()-t2, t2-t1))\n",
        "    # перекодировка\n",
        "    encoded_alphabet = list(range(list(alphabet)))\n",
        "    recoding = {a: code for code, a in enumerate(alphabet)}\n",
        "    recoded_words = [[]]\n",
        "\n",
        "def test_precomputing_symbols():\n",
        "    alphabet = 'абвгдеёжзийклмнопрстуфхцчшщьыъэюя-'\n",
        "    infile = \"test_data/words_100000.txt\"\n",
        "    words = []\n",
        "    with open(infile, \"r\", encoding=\"utf8\") as fin:\n",
        "        for line in fin:\n",
        "            line = line.strip().lower()\n",
        "            if len(line) != 0:\n",
        "                words.append(line)\n",
        "    tm = TrieMinimizer()\n",
        "    trie = Trie(alphabet, is_numpied=False, precompute_symbols=2)\n",
        "    trie.fit(words[:10])\n",
        "    compressed, node_classes =\\\n",
        "        tm.minimize(trie, precompute_symbols=2, return_groups=True)\n",
        "    possible_continuations = [set() for _ in compressed.graph]\n",
        "    for future_symbols, index in zip(trie.data, node_classes):\n",
        "        possible_continuations[index].add(\"|\".join(\n",
        "            \",\".join(map(str, sorted(elem))) for elem in future_symbols))\n",
        "    compressed_continuations =\\\n",
        "        [\"|\".join(\",\".join(map(str, sorted(elem))) for elem in future_symbols)\n",
        "         for future_symbols in compressed.data]\n",
        "    print(sum(int(len(x) > 1) for x in possible_continuations))\n",
        "    print(sum((list(x)[0] != y) for x, y in\n",
        "              zip(possible_continuations, compressed_continuations)))"
      ],
      "metadata": {
        "id": "QbjjObqO28y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Trie:\n",
        "    '''\n",
        " Реализация префиксного бора (точнее, корневого направленного ациклического графа)\n",
        " Атрибуты\n",
        "    --------\n",
        " alphabet: list, алфавит\n",
        " alphabet_codes: dict, словарь символ:код\n",
        " compressed: bool, индикатор сжатия\n",
        " cashed: bool, индикатор кэширования запросов к функции descend\n",
        " root: int, индекс корня\n",
        " graph: array, type=int, shape=(число вершин, размер алфавита), матрица потомков\n",
        "    graph[i][j] = k <-> вершина k --- потомок вершины i по ребру, помеченному символом alphabet[j]\n",
        "    data: array, type=object, shape=(число вершин), массив с данными, хранящямися в вершинах\n",
        "    final: array, type=bool, shape=(число вершин), массив индикаторов\n",
        "    final[i] = True <-> i --- финальная вершина\n",
        "    '''\n",
        "    NO_NODE = -1\n",
        "    SPACE_CODE = -1\n",
        "\n",
        "    ATTRS = ['is_numpied', 'precompute_symbols', 'allow_spaces',\n",
        "             'is_terminated', 'to_make_cashed']\n",
        "\n",
        "    def __init__(self, alphabet, make_sorted=True, make_alphabet_codes=True,\n",
        "                 is_numpied=False, to_make_cashed=False,\n",
        "                 precompute_symbols=None, allow_spaces=False, dict_storage=False):\n",
        "        self.alphabet = sorted(alphabet) if make_sorted else alphabet\n",
        "        self.alphabet_codes = ({a: i for i, a in enumerate(self.alphabet)}\n",
        "                               if make_alphabet_codes else self.alphabet)\n",
        "        self.alphabet_codes[\" \"] = Trie.SPACE_CODE\n",
        "        self.is_numpied = is_numpied\n",
        "        self.to_make_cashed = to_make_cashed\n",
        "        self.dict_storage = dict_storage\n",
        "        self.precompute_symbols = precompute_symbols\n",
        "        self.allow_spaces = allow_spaces\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        self.root = 0\n",
        "        self.graph = [self._make_default_node()]\n",
        "        self.data, self.final = [None], [False]\n",
        "        self.nodes_number = 1\n",
        "        self.descend = self._descend_simple\n",
        "        self.is_terminated = False\n",
        "\n",
        "    def _make_default_node(self):\n",
        "        if self.dict_storage:\n",
        "            return defaultdict(lambda: -1)\n",
        "        elif self.is_numpied:\n",
        "            return np.full(shape=(len(self.alphabet),),\n",
        "                           fill_value=Trie.NO_NODE, dtype=int)\n",
        "        else:\n",
        "            return [Trie.NO_NODE] * len(self.alphabet)\n",
        "\n",
        "    def save(self, outfile):\n",
        "        \"\"\"\n",
        "        Сохраняет дерево для дальнейшего использования\n",
        "        \"\"\"\n",
        "        with open(outfile, \"w\", encoding=\"utf8\") as fout:\n",
        "            attr_values = [getattr(self, attr) for attr in Trie.ATTRS]\n",
        "            attr_values.append(any(x is not None for x in self.data))\n",
        "            fout.write(\"{}\\n{}\\t{}\\n\".format(\n",
        "                \" \".join(\"T\" if x else \"F\" for x in attr_values),\n",
        "                self.nodes_number, self.root))\n",
        "            fout.write(\" \".join(str(a) for a in self.alphabet) + \"\\n\")\n",
        "            for index, label in enumerate(self.final):\n",
        "                letters = self._get_letters(index, return_indexes=True)\n",
        "                children = self._get_children(index)\n",
        "                fout.write(\"{}\\t{}\\n\".format(\n",
        "                    \"T\" if label else \"F\", \" \".join(\"{}:{}\".format(*elem)\n",
        "                                                    for elem in zip(letters, children))))\n",
        "            if self.precompute_symbols is not None:\n",
        "                for elem in self.data:\n",
        "                    fout.write(\":\".join(\",\".join(\n",
        "                        map(str, symbols)) for symbols in elem) + \"\\n\")\n",
        "        return\n",
        "\n",
        "    def make_cashed(self):\n",
        "        '''\n",
        "        Включает кэширование запросов к descend\n",
        "        '''\n",
        "        self._descendance_cash = [dict() for _ in self.graph]\n",
        "        self.descend = self._descend_cashed\n",
        "\n",
        "    def make_numpied(self):\n",
        "        self.graph = np.array(self.graph)\n",
        "        self.final = np.asarray(self.final, dtype=bool)\n",
        "        self.is_numpied = True\n",
        "\n",
        "    def add(self, s):\n",
        "        '''\n",
        "        Добавление строки s в префиксный бор\n",
        "        '''\n",
        "        if self.is_terminated:\n",
        "            raise TypeError(\"Impossible to add string to fitted trie\")\n",
        "        if s == \"\":\n",
        "            self._set_final(self.root)\n",
        "            return\n",
        "        curr = self.root\n",
        "        for i, a in enumerate(s):\n",
        "            code = self.alphabet_codes[a]\n",
        "            next = self.graph[curr][code]\n",
        "            if next == Trie.NO_NODE:\n",
        "                curr = self._add_descendant(curr, s[i:])\n",
        "                break\n",
        "            else:\n",
        "                curr = next\n",
        "        self._set_final(curr)\n",
        "        return self\n",
        "\n",
        "    def fit(self, words):\n",
        "        for s in words:\n",
        "            self.add(s)\n",
        "        self.terminate()\n",
        "\n",
        "    def terminate(self):\n",
        "        if self.is_numpied:\n",
        "            self.make_numpied()\n",
        "        self.terminated = True\n",
        "        if self.precompute_symbols is not None:\n",
        "            precompute_future_symbols(self, self.precompute_symbols,\n",
        "                                      allow_spaces=self.allow_spaces)\n",
        "        if self.to_make_cashed:\n",
        "            self.make_cashed()\n",
        "\n",
        "    def __contains__(self, s):\n",
        "        if any(a not in self.alphabet for a in s):\n",
        "            return False\n",
        "        # word = tuple(self.alphabet_codes[a] for a in s)\n",
        "        node = self.descend(self.root, s)\n",
        "        return (node != Trie.NO_NODE) and self.is_final(node)\n",
        "\n",
        "    def words(self):\n",
        "        \"\"\"\n",
        "        Возвращает итератор по словам, содержащимся в боре\n",
        "        \"\"\"\n",
        "        branch, word, indexes = [self.root], [], [0]\n",
        "        letters_with_children = [self._get_children_and_letters(self.root)]\n",
        "        while len(branch) > 0:\n",
        "            if self.is_final(branch[-1]):\n",
        "                yield \"\".join(word)\n",
        "            while indexes[-1] == len(letters_with_children[-1]):\n",
        "                indexes.pop()\n",
        "                letters_with_children.pop()\n",
        "                branch.pop()\n",
        "                if len(indexes) == 0:\n",
        "                    raise StopIteration()\n",
        "                word.pop()\n",
        "            next_letter, next_child = letters_with_children[-1][indexes[-1]]\n",
        "            indexes[-1] += 1\n",
        "            indexes.append(0)\n",
        "            word.append(next_letter)\n",
        "            branch.append(next_child)\n",
        "            letters_with_children.append(self._get_children_and_letters(branch[-1]))\n",
        "\n",
        "    def is_final(self, index):\n",
        "        '''\n",
        "        Аргументы\n",
        "        ---------\n",
        "        index: int, номер вершины\n",
        "        Возвращает\n",
        "        ----------\n",
        "        True: если index --- номер финальной вершины\n",
        "        '''\n",
        "        return self.final[index]\n",
        "\n",
        "    def find_substrings(self, s, return_positions=False, return_compressed=True):\n",
        "        \"\"\"\n",
        "        Finds all nonempty substrings of s in the trie\n",
        "        \"\"\"\n",
        "        curr_agenda = {self.root: {0}}\n",
        "        answer = [[] for _ in s]\n",
        "        for i, a in enumerate(s, 1):\n",
        "            next_agenda = defaultdict(set)\n",
        "            for curr, starts in curr_agenda.items():\n",
        "                if a in self.alphabet:\n",
        "                    child = self.graph[curr][self.alphabet_codes[a]]\n",
        "                    if child == Trie.NO_NODE:\n",
        "                        continue\n",
        "                    next_agenda[child] |= starts\n",
        "            next_agenda[self.root].add(i)\n",
        "            for curr, starts in next_agenda.items():\n",
        "                 if self.is_final(curr):\n",
        "                     answer[i-1].extend(starts)\n",
        "            curr_agenda = next_agenda\n",
        "        answer = [(x, i) for i, x in enumerate(answer, 1)]\n",
        "        if not return_positions or not return_compressed:\n",
        "            answer = [(i, j) for starts, j in answer for i in starts]\n",
        "        if not return_positions:\n",
        "            answer = [s[i:j] for i, j in answer]\n",
        "        return answer\n",
        "    def find_partitions(self, s, max_count=1):\n",
        "        \"\"\"\n",
        "        Находит все разбиения s = s_1 ... s_m на словарные слова s_1, ..., s_m\n",
        "        для m <= max_count\n",
        "        \"\"\"\n",
        "        curr_agenda = [(self.root, [], 0)]\n",
        "        for i, a in enumerate(s):\n",
        "            next_agenda = []\n",
        "            for curr, borders, cost in curr_agenda:\n",
        "                if cost >= max_count:\n",
        "                    continue\n",
        "                child = self.graph[curr][self.alphabet_codes[a]]\n",
        "                # child = self.graph[curr][a]\n",
        "                if child == Trie.NO_NODE:\n",
        "                    continue\n",
        "                next_agenda.append((child, borders, cost))\n",
        "                if self.is_final(child):\n",
        "                    next_agenda.append((self.root, borders + [i+1], cost+1))\n",
        "            curr_agenda = next_agenda\n",
        "        answer = []\n",
        "        for curr, borders, cost in curr_agenda:\n",
        "            if curr == self.root:\n",
        "                borders = [0] + borders\n",
        "                answer.append([s[left:borders[i+1]] for i, left in enumerate(borders[:-1])])\n",
        "        return answer\n",
        "\n",
        "    def _get_accepting_prefixes_lengths(self, s, start=None):\n",
        "        if start is None:\n",
        "            start = self.root\n",
        "        answer = []\n",
        "        for i, symbol in enumerate(s, 1):\n",
        "            code = self.alphabet_codes.get(symbol)\n",
        "            if code is None:\n",
        "                break\n",
        "            start = self.graph[start][code]\n",
        "            if start == self.NO_NODE:\n",
        "                break\n",
        "            if self.is_final(start):\n",
        "                answer.append(i)\n",
        "        return answer\n",
        "\n",
        "    def descend_by_prefixes(self, s, max_count=1, start_pos=0, start_node=None, return_pairs=False):\n",
        "        if start_node is None:\n",
        "            start_node = self.root\n",
        "        if isinstance(start_pos, int):\n",
        "            start_pos = [start_pos]\n",
        "        start_pos = sorted(start_pos)\n",
        "        start = start_pos[0]\n",
        "        if max_count == 1 and len(start_pos) == 1:\n",
        "            answer = self._get_accepting_prefixes_lengths(s[start:], start=start_node)\n",
        "            if return_pairs:\n",
        "                answer = [(start, start+k) for k in answer]\n",
        "            else:\n",
        "                answer = [start+k for k in answer]\n",
        "            return answer\n",
        "        answer = set()\n",
        "        curr_agenda = {start_node: {start: 1}}\n",
        "        for i, symbol in enumerate(s[start:], start):\n",
        "            code = self.alphabet_codes.get(symbol)\n",
        "            if code is None:\n",
        "                break\n",
        "            if i in start_pos[1:]:\n",
        "                curr_agenda[start_node][i] = 1\n",
        "            new_agenda = defaultdict(dict)\n",
        "            for curr, starts_with_ranks in curr_agenda.items():\n",
        "                curr = self.graph[curr][code]\n",
        "                if curr == self.NO_NODE:\n",
        "                    continue\n",
        "                is_final = self.is_final(curr)\n",
        "                for start, rank in starts_with_ranks.items():\n",
        "                    if start not in new_agenda[curr] or rank < new_agenda[curr][start]:\n",
        "                        new_agenda[curr][start] = rank\n",
        "                    if is_final:\n",
        "                        answer.add((start, i+1))\n",
        "                        if rank < max_count:\n",
        "                            if i+1 not in new_agenda[self.root] or rank + 1 < new_agenda[self.root][i+1]:\n",
        "                                new_agenda[self.root][i + 1] = rank + 1\n",
        "            curr_agenda = new_agenda\n",
        "        if not return_pairs:\n",
        "            answer = {elem[1] for elem in answer}\n",
        "        return sorted(answer)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nodes_number\n",
        "\n",
        "    def __repr__(self):\n",
        "        answer = \"\"\n",
        "        for i, (final, data) in enumerate(zip(self.final, self.data)):\n",
        "            letters, children = self._get_letters(i), self._get_children(i)\n",
        "            answer += \"{0}\".format(i)\n",
        "            if final:\n",
        "                answer += \"F\"\n",
        "            for a, index in zip(letters, children):\n",
        "                answer += \" {0}:{1}\".format(a, index)\n",
        "            answer += \"\\n\"\n",
        "            if data is not None:\n",
        "                answer += \"data:{0} {1}\\n\".format(len(data), \" \".join(str(elem) for elem in data))\n",
        "        return answer\n",
        "\n",
        "    def _add_descendant(self, parent, s, final=False):\n",
        "        for a in s:\n",
        "            code = self.alphabet_codes[a]\n",
        "            parent = self._add_empty_child(parent, code, final)\n",
        "        return parent\n",
        "\n",
        "    def _add_empty_child(self, parent, code, final=False):\n",
        "        '''\n",
        "        Добавление ребёнка к вершине parent по символу с кодом code\n",
        "        '''\n",
        "        self.graph[parent][code] = self.nodes_number\n",
        "        self.graph.append(self._make_default_node())\n",
        "        self.data.append(None)\n",
        "        self.final.append(final)\n",
        "        self.nodes_number += 1\n",
        "        return (self.nodes_number - 1)\n",
        "\n",
        "    def _descend_simple(self, curr, s):\n",
        "        '''\n",
        "        Спуск из вершины curr по строке s\n",
        "        '''\n",
        "        for a in s:\n",
        "            curr = self.graph[curr][self.alphabet_codes[a]]\n",
        "            if curr == Trie.NO_NODE:\n",
        "                break\n",
        "        return curr\n",
        "\n",
        "    def _descend_cashed(self, curr, s):\n",
        "        '''\n",
        "        Спуск из вершины curr по строке s с кэшированием\n",
        "        '''\n",
        "        if s == \"\":\n",
        "            return curr\n",
        "        curr_cash = self._descendance_cash[curr]\n",
        "        answer = curr_cash.get(s, None)\n",
        "        if answer is not None:\n",
        "            return answer\n",
        "        # для оптимизации дублируем код\n",
        "        res = curr\n",
        "        for a in s:\n",
        "            res = self.graph[res][self.alphabet_codes[a]]\n",
        "            # res = self.graph[res][a]\n",
        "            if res == Trie.NO_NODE:\n",
        "                break\n",
        "        curr_cash[s] = res\n",
        "        return res\n",
        "\n",
        "    def _set_final(self, curr):\n",
        "        '''\n",
        "        Делает состояние curr завершающим\n",
        "        '''\n",
        "        self.final[curr] = True\n",
        "\n",
        "    def _get_letters(self, index, return_indexes=False):\n",
        "        \"\"\"\n",
        "        Извлекает все метки выходных рёбер вершины с номером index\n",
        "        \"\"\"\n",
        "        if self.dict_storage:\n",
        "            answer = list(self.graph[index].keys())\n",
        "        else:\n",
        "            answer =  [i for i, elem in enumerate(self.graph[index])\n",
        "                       if elem != Trie.NO_NODE]\n",
        "        if not return_indexes:\n",
        "            answer = [(self.alphabet[i] if i >= 0 else \" \") for i in answer]\n",
        "        return answer\n",
        "\n",
        "    def _get_children_and_letters(self, index, return_indexes=False):\n",
        "        if self.dict_storage:\n",
        "            answer = list(self.graph[index].items())\n",
        "        else:\n",
        "            answer =  [elem for elem in enumerate(self.graph[index])\n",
        "                       if elem[1] != Trie.NO_NODE]\n",
        "        if not return_indexes:\n",
        "            for i, (letter_index, child) in enumerate(answer):\n",
        "                answer[i] = (self.alphabet[letter_index], child)\n",
        "        return answer\n",
        "\n",
        "    def _get_children(self, index):\n",
        "        \"\"\"\n",
        "        Извлекает всех потомков вершины с номером index\n",
        "        \"\"\"\n",
        "        if self.dict_storage:\n",
        "            return list(self.graph[index].values())\n",
        "        else:\n",
        "            return [elem for elem in self.graph[index] if elem != Trie.NO_NODE]"
      ],
      "metadata": {
        "id": "xO0LJuCJ2Re_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_trie(words, alphabet=None, compressed=True, is_numpied=False,\n",
        "              make_cashed=False, precompute_symbols=False,\n",
        "              allow_spaces=False, dict_storage=False):\n",
        "    if alphabet is None:\n",
        "        alphabet = sorted({x for word in words for x in word})\n",
        "    trie = Trie(alphabet, is_numpied=is_numpied, to_make_cashed=make_cashed,\n",
        "                precompute_symbols=precompute_symbols, dict_storage=dict_storage)\n",
        "    trie.fit(words)\n",
        "    print(len(trie))\n",
        "    if compressed:\n",
        "        tm = TrieMinimizer()\n",
        "        trie = tm.minimize(trie, dict_storage=dict_storage, make_cashed=make_cashed,\n",
        "                           make_numpied=is_numpied, precompute_symbols=precompute_symbols,\n",
        "                           allow_spaces=allow_spaces)\n",
        "        print(len(trie))\n",
        "    return trie\n",
        "\n"
      ],
      "metadata": {
        "id": "lrzdNLunZd-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_config(infile):\n",
        "    with open(infile, \"r\", encoding=\"utf8\") as fin:\n",
        "        config = json.load(fin)\n",
        "    if \"use_morpheme_types\" not in config:\n",
        "        config[\"use_morpheme_types\"] = True\n",
        "    return config\n",
        "\n",
        "# вспомогательные фунцкии\n",
        "\n",
        "def to_one_hot(data, classes_number):\n",
        "    answer = np.eye(classes_number, dtype=np.uint8)\n",
        "    return answer[data]\n",
        "\n",
        "def make_model_file(name, i):\n",
        "    pos = name.rfind(\".\")\n",
        "    if pos != -1:\n",
        "        return \"{}-{}.{}\".format(name[:pos], i, name[pos+1:])\n",
        "    else:\n",
        "        return \"{}-{}\".format(name, i)\n"
      ],
      "metadata": {
        "id": "BFuxXlAQbCKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUXILIARY_CODES = PAD, BEGIN, END, UNKNOWN = 0, 1, 2, 3\n",
        "AUXILIARY = ['PAD', 'BEGIN', 'END', 'UNKNOWN']\n",
        "\n",
        "\n",
        "def _make_vocabulary(source):\n",
        "    \"\"\"\n",
        "    Создаёт словарь символов.\n",
        "    \"\"\"\n",
        "    symbols = {a for word in source for a in word}\n",
        "    symbols = AUXILIARY + sorted(symbols)\n",
        "    symbol_codes = {a: i for i, a in enumerate(symbols)}\n",
        "    return symbols, symbol_codes\n",
        "\n",
        "def make_bucket_lengths(lengths, buckets_number):\n",
        "    \"\"\"\n",
        "    Вычисляет максимальные длины элементов в корзинах. Каждая корзина состоит из элементов примерно одинаковой длины\n",
        "    \"\"\"\n",
        "    m = len(lengths)\n",
        "    lengths = sorted(lengths)\n",
        "    last_bucket_length, bucket_lengths = 0, []\n",
        "    for i in range(buckets_number):\n",
        "        # могут быть проблемы с выбросами большой длины\n",
        "        level = (m * (i + 1) // buckets_number) - 1\n",
        "        curr_length = lengths[level]\n",
        "        if curr_length > last_bucket_length:\n",
        "            bucket_lengths.append(curr_length)\n",
        "            last_bucket_length = curr_length\n",
        "    return bucket_lengths"
      ],
      "metadata": {
        "id": "SMGU_dWbbO1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_buckets(lengths, buckets_number, max_bucket_size=-1):\n",
        "    \"\"\"\n",
        "    Распределяет элементы по корзинам\n",
        "    \"\"\"\n",
        "    bucket_lengths = make_bucket_lengths(lengths, buckets_number)\n",
        "    indexes = [[] for _ in bucket_lengths]\n",
        "    for i, length in enumerate(lengths):\n",
        "        index = bisect.bisect_left(bucket_lengths, length)\n",
        "        indexes[index].append(i)\n",
        "    if max_bucket_size != -1:\n",
        "        bucket_lengths = list(chain.from_iterable(\n",
        "            ([L] * ((len(curr_indexes)-1) // max_bucket_size + 1))\n",
        "            for L, curr_indexes in zip(bucket_lengths, indexes)\n",
        "            if len(curr_indexes) > 0))\n",
        "        indexes = [curr_indexes[start:start+max_bucket_size]\n",
        "                   for curr_indexes in indexes\n",
        "                   for start in range(0, len(curr_indexes), max_bucket_size)]\n",
        "    return [(L, curr_indexes) for L, curr_indexes\n",
        "            in zip(bucket_lengths, indexes) if len(curr_indexes) > 0]\n",
        "\n",
        "def load_cls(infile):\n",
        "    with open(infile, \"r\", encoding=\"utf8\") as fin:\n",
        "        json_data = json.load(fin)\n",
        "    args = {key: value for key, value in json_data.items()\n",
        "            if not (key.endswith(\"_\") or key.endswith(\"callback\") or key == \"model_files\")}\n",
        "    args['callbacks'] = []\n",
        "    # создаём классификатор\n",
        "    inflector = Partitioner(**args)\n",
        "    # обучаемые параметры\n",
        "    args = {key: value for key, value in json_data.items() if key[-1] == \"_\"}\n",
        "    for key, value in args.items():\n",
        "        setattr(inflector, key, value)\n",
        "    if hasattr(inflector, \"morphemes_\"):\n",
        "        inflector._make_morpheme_tries()\n",
        "    # модель\n",
        "    inflector.build()  # не работает сохранение/загрузка модели, приходится перекомпилировать\n",
        "    for i, (model, model_file) in enumerate(\n",
        "            zip(inflector.models_, json_data['model_files'])):\n",
        "        model.load_weights(model_file)\n",
        "    return inflector\n"
      ],
      "metadata": {
        "id": "Ze7saJ4NbXfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MORPHEME_TYPES = [\"PREF\", \"ROOT\", \"LINK\", \"END\", \"POSTFIX\", \"HYPH\"]\n",
        "PREF, ROOT, LINK, SUFF, ENDING, POSTFIX, HYPH, FINAL = 0, 1, 2, 3, 4, 5, 6, 7"
      ],
      "metadata": {
        "id": "Z2ZiFd-clr9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_morpheme_types(morpheme_type):\n",
        "    \"\"\"\n",
        "    Определяет, какие морфемы могут идти за текущей.\n",
        "    \"\"\"\n",
        "    if morpheme_type == \"None\":\n",
        "        return [\"None\"]\n",
        "    MORPHEMES = [\"SUFF\", \"END\", \"LINK\", \"POSTFIX\", \"PREF\", \"ROOT\"]\n",
        "    if morpheme_type in [\"ROOT\", \"SUFF\", \"HYPH\"]:\n",
        "        start = 0\n",
        "    elif morpheme_type == \"END\":\n",
        "        start = 2\n",
        "    elif morpheme_type in [\"PREF\", \"LINK\", \"BEGIN\"]:\n",
        "        start = 4\n",
        "    else:\n",
        "        start = 6\n",
        "    answer = MORPHEMES[start:6]\n",
        "    if len(answer) > 0 and morpheme_type != \"HYPH\":\n",
        "        answer.append(\"HYPH\")\n",
        "    if morpheme_type == \"BEGIN\":\n",
        "        answer.append(\"None\")\n",
        "    return answer"
      ],
      "metadata": {
        "id": "0WuCLCa1mWEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_morpheme(morpheme):\n",
        "    \"\"\"\n",
        "    Строит список меток, которые могут идти за текущей\n",
        "    \"\"\"\n",
        "    if morpheme == \"BEGIN\":\n",
        "        morpheme = \"S-BEGIN\"\n",
        "    morpheme_label, morpheme_type = morpheme.split(\"-\")\n",
        "    if morpheme_label in \"BM\":\n",
        "        new_morpheme_labels = \"ME\"\n",
        "        new_morpheme_types = [morpheme_type]\n",
        "    else:\n",
        "        new_morpheme_labels = \"BS\"\n",
        "        new_morpheme_types = get_next_morpheme_types(morpheme_type)\n",
        "    answer = [\"{}-{}\".format(x, y) for x in new_morpheme_labels for y in new_morpheme_types]\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "zo3MIPpvo6ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_correct_morpheme_sequence(morphemes):\n",
        "    \"\"\"\n",
        "    Проверяет список морфемных меток на корректность\n",
        "    \"\"\"\n",
        "    if morphemes == []:\n",
        "        return False\n",
        "    if any(\"-\" not in morpheme for morpheme in morphemes):\n",
        "        return False\n",
        "    morpheme_label, morpheme_type = morphemes[0].split(\"-\")\n",
        "    if morpheme_label not in \"BS\" or morpheme_type not in [\"PREF\", \"ROOT\", \"None\"]:\n",
        "        return False\n",
        "    morpheme_label, morpheme_type = morphemes[-1].split(\"-\")\n",
        "    if morpheme_label not in \"ES\" or morpheme_type not in [\"ROOT\", \"SUFF\", \"ENDING\", \"POSTFIX\", \"None\"]:\n",
        "        return False\n",
        "    for i, morpheme in enumerate(morphemes[:-1]):\n",
        "        if morphemes[i+1] not in get_next_morpheme(morpheme):\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "ruFDHyHNpJKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Partitioner:\n",
        "    LEFT_MORPHEME_TYPES = [\"pref\", \"root\"]\n",
        "    RIGHT_MORPHEME_TYPES = [\"root\", \"suff\", \"end\", \"postfix\"]\n",
        "    def __init__(self, models_number=1, use_morpheme_types=True,\n",
        "                 to_memorize_morphemes=False, min_morpheme_count=2,\n",
        "                 to_memorize_ngram_counts=False, min_relative_ngram_count=0.1,\n",
        "                 use_embeddings=False, embeddings_size=32,\n",
        "                 conv_layers=1, window_size=5, filters_number=64,\n",
        "                 dense_output_units=0, use_lstm=False, lstm_units=64,\n",
        "                 dropout=0.0, context_dropout=0.0,\n",
        "                 buckets_number=10, nepochs=10,\n",
        "                 validation_split=0.2, batch_size=32,\n",
        "                 callbacks=None, early_stopping=None):\n",
        "        self.models_number = models_number\n",
        "        self.use_morpheme_types = use_morpheme_types\n",
        "        self.to_memorize_morphemes = to_memorize_morphemes\n",
        "        self.min_morpheme_count = min_morpheme_count\n",
        "        self.to_memorize_ngram_counts = to_memorize_ngram_counts\n",
        "        self.min_relative_ngram_count = min_relative_ngram_count\n",
        "        self.use_embeddings = use_embeddings\n",
        "        self.embeddings_size = embeddings_size\n",
        "        self.conv_layers = conv_layers\n",
        "        self.window_size = window_size\n",
        "        self.filters_number = filters_number\n",
        "        self.dense_output_units = dense_output_units\n",
        "        self.use_lstm = use_lstm\n",
        "        self.lstm_units = lstm_units\n",
        "        self.dropout = dropout\n",
        "        self.context_dropout = context_dropout\n",
        "        self.buckets_number = buckets_number\n",
        "        self.nepochs = nepochs\n",
        "        self.validation_split = validation_split\n",
        "        self.batch_size = batch_size\n",
        "        self.callbacks = callbacks\n",
        "        self.early_stopping = early_stopping\n",
        "        self.check_params()\n",
        "\n",
        "    def check_params(self):\n",
        "        if isinstance(self.window_size, int):\n",
        "            # если было только одно окно в свёрточных слоях\n",
        "            self.window_size = [self.window_size]\n",
        "        # приводим фильтры к двумерному виду\n",
        "        self.filters_number = np.atleast_2d(self.filters_number)\n",
        "        if self.filters_number.shape[0] == 1:\n",
        "            self.filters_number = np.repeat(self.filters_number, len(self.window_size), axis=0)\n",
        "        if self.filters_number.shape[0] != len(self.window_size):\n",
        "            raise ValueError(\"Filters array should have shape (len(window_size), conv_layers)\")\n",
        "        if self.filters_number.shape[1] == 1:\n",
        "            self.filters_number = np.repeat(self.filters_number, self.conv_layers, axis=1)\n",
        "        if self.filters_number.shape[1] != self.conv_layers:\n",
        "            raise ValueError(\"Filters array should have shape (len(window_size), conv_layers)\")\n",
        "        # переводим в список из int, а не np.int32, чтобы не было проблем при сохранении\n",
        "        self.filters_number = list([list(map(int, x)) for x in self.filters_number])\n",
        "        if self.callbacks is None:\n",
        "            self.callbacks = []\n",
        "        if (self.early_stopping is not None and\n",
        "                not any(isinstance(x, EarlyStopping) for x in self.callbacks)):\n",
        "            self.callbacks.append(EarlyStopping(patience=self.early_stopping, monitor=\"val_acc\"))\n",
        "        if self.use_morpheme_types:\n",
        "            self._morpheme_memo_func = self._make_morpheme_data\n",
        "        else:\n",
        "            self._morpheme_memo_func = self._make_morpheme_data_simple\n",
        "\n",
        "    def to_json(self, outfile, model_file=None):\n",
        "        info = dict()\n",
        "        if model_file is None:\n",
        "            pos = outfile.rfind(\".\")\n",
        "            model_file = outfile[:pos] + (\"-model.hdf5\" if pos != -1 else \"-model\")\n",
        "        model_files = [make_model_file(model_file, i+1) for i in range(self.models_number)]\n",
        "        for i in range(self.models_number):\n",
        "            # при сохранении нужен абсолютный путь, а не от текущей директории\n",
        "            model_files[i] = os.path.abspath(model_files[i])\n",
        "        for (attr, val) in inspect.getmembers(self):\n",
        "            # перебираем поля класса и сохраняем только задаваемые при инициализации\n",
        "            if not (attr.startswith(\"__\") or inspect.ismethod(val) or\n",
        "                    isinstance(getattr(Partitioner, attr, None), property) or\n",
        "                    attr.isupper() or attr in [\n",
        "                        \"callbacks\", \"models_\", \"left_morphemes_\", \"right_morphemes_\", \"morpheme_trie_\"]):\n",
        "                info[attr] = val\n",
        "            elif attr == \"models_\":\n",
        "                # для каждой модели сохраняем веса\n",
        "                info[\"model_files\"] = model_files\n",
        "                for model, curr_model_file in zip(self.models_, model_files):\n",
        "                    model.save_weights(curr_model_file)\n",
        "        with open(outfile, \"w\", encoding=\"utf8\") as fout:\n",
        "            json.dump(info, fout)\n",
        "\n",
        "    # property --- функция, прикидывающаяся переменной; декоратор метода (превращает метод класса в атрибут класса)\n",
        "    @property \n",
        "    def symbols_number_(self):\n",
        "        return len(self.symbols_)\n",
        "\n",
        "    @property\n",
        "    def target_symbols_number_(self):\n",
        "        return len(self.target_symbols_)\n",
        "\n",
        "    @property\n",
        "    def memory_dim(self):\n",
        "        return 15 if self.use_morpheme_types else 3\n",
        "\n",
        "    def _preprocess(self, data, targets=None):\n",
        "        # к каждому слову добавляются символы начала и конца строки\n",
        "        lengths = [len(x) + 2 for x in data]\n",
        "        # разбиваем данные на корзины\n",
        "        buckets_with_indexes = collect_buckets(lengths, self.buckets_number)\n",
        "        # преобразуем данные в матрицы в каждой корзине\n",
        "        data_by_buckets = [self._make_bucket_data(data, length, indexes)\n",
        "                           for length, indexes in buckets_with_indexes]\n",
        "        # targets=None --- предсказание, иначе --- обучение\n",
        "        if targets is not None:\n",
        "            targets_by_buckets = [self._make_bucket_data(targets, length, indexes, is_target=True)\n",
        "                                  for length, indexes in buckets_with_indexes]\n",
        "            return data_by_buckets, targets_by_buckets, buckets_with_indexes\n",
        "        else:\n",
        "            return data_by_buckets, buckets_with_indexes\n",
        "\n",
        "    def _make_bucket_data(self, data, bucket_length, bucket_indexes, is_target=False):\n",
        "        \"\"\"\n",
        "        data: list of lists, исходные данные\n",
        "        bucket_length: int, максимальная длина элемента в корзине\n",
        "        bucket_indexes: list of ints, индексы элементов в корзине\n",
        "        is_target: boolean, default=False,\n",
        "            являются ли данные исходными или ответами\n",
        "        answer = [symbols, (classes)],\n",
        "            symbols: array of shape (len(data), bucket_length)\n",
        "                элементы data, дополненные символом PAD справа до bucket_length\n",
        "            classes: array of shape (len(data), classes_number)\n",
        "        \"\"\"\n",
        "        bucket_data = [data[i] for i in bucket_indexes]\n",
        "        if is_target:\n",
        "            return self._recode_bucket_data(bucket_data, bucket_length, self.target_symbol_codes_)\n",
        "        else:\n",
        "            answer = [self._recode_bucket_data(bucket_data, bucket_length, self.symbol_codes_)]\n",
        "            if self.to_memorize_morphemes:\n",
        "                print(\"Processing morphemes for bucket length\", bucket_length)\n",
        "                answer.append(self._morpheme_memo_func(bucket_data, bucket_length))\n",
        "                print(\"Processing morphemes for bucket length\", bucket_length, \"finished\")\n",
        "            return answer\n",
        "\n",
        "    def _recode_bucket_data(self, data, bucket_length, encoding):\n",
        "        answer = np.full(shape=(len(data), bucket_length), fill_value=PAD, dtype=int)\n",
        "        answer[:,0] = BEGIN\n",
        "        for j, word in enumerate(data):\n",
        "          try:\n",
        "            answer[j,1:1+len(word)] = [encoding.get(x, UNKNOWN) for x in word]\n",
        "            answer[j,1+len(word)] = END\n",
        "          except:\n",
        "            pass\n",
        "        return answer\n",
        "\n",
        "    def _make_morpheme_data(self, data, bucket_length):\n",
        "        \"\"\"\n",
        "        строит для каждой позиции во входных словах вектор, кодирующий энграммы в контексте\n",
        "        data: list of strs, список исходных слов\n",
        "        bucket_length: int, максимальная длина слова в корзине\n",
        "        answer: np.array[float] of shape (len(data), bucket_length, 15)\n",
        "        \"\"\"\n",
        "        answer = np.zeros(shape=(len(data), bucket_length, 15), dtype=float)\n",
        "        for j, word in enumerate(data):\n",
        "            m = len(word)\n",
        "            curr_answer = np.zeros(shape=(bucket_length, 15), dtype=int)\n",
        "            root_starts = [0]\n",
        "            ending_ends = [m]\n",
        "            prefixes = self.left_morphemes_[\"pref\"].descend_by_prefixes(word[:-1])\n",
        "            for end in prefixes:\n",
        "                score = self._get_ngram_score(word[:end], \"pref\")\n",
        "                if end == 1:\n",
        "                    curr_answer[1,10] = max(score, curr_answer[1,10])\n",
        "                else:\n",
        "                    curr_answer[1,0] = max(score, curr_answer[1,0])\n",
        "                    curr_answer[end, 5] = max(score, curr_answer[end, 5])\n",
        "            root_starts += prefixes\n",
        "            postfix_lengths = self.right_morphemes_[\"postfix\"].descend_by_prefixes(word[:0:-1])\n",
        "            for k in postfix_lengths:\n",
        "                score = self._get_ngram_score(word[-k:], \"postfix\")\n",
        "                if k == 1:\n",
        "                    curr_answer[m, 14] = max(score, curr_answer[m, 14])\n",
        "                else:\n",
        "                    curr_answer[m, 9] = max(score, curr_answer[m, 9])\n",
        "                    curr_answer[m-k+1,4] = max(score, curr_answer[m-k+1,4])\n",
        "                ending_ends.append(m-k)\n",
        "            suffix_ends = set(ending_ends)\n",
        "            for end in ending_ends[::-1]:\n",
        "                ending_lengths = self.right_morphemes_[\"end\"].descend_by_prefixes(word[end-1:0:-1])\n",
        "                for k in ending_lengths:\n",
        "                    score = self._get_ngram_score(word[end-k:end], \"end\")\n",
        "                    if k == 1:\n",
        "                        curr_answer[end, 13] = max(score, curr_answer[end, 13])\n",
        "                    else:\n",
        "                        curr_answer[end-k+1, 3] = max(score, curr_answer[end-k+1, 3])\n",
        "                        curr_answer[end, 8] = max(score, curr_answer[end, 8])\n",
        "                    suffix_ends.add(end-k)\n",
        "            suffixes = self.right_morphemes_[\"suff\"].descend_by_prefixes(\n",
        "                word[::-1], start_pos=[m-k for k in suffix_ends], max_count=3, return_pairs=True)\n",
        "            suffix_starts = suffix_ends\n",
        "            for first, last in suffixes:\n",
        "                score = self._get_ngram_score(word[m-last:m-first], \"suff\")\n",
        "                if last == first + 1:\n",
        "                    curr_answer[m-first, 12] = max(score, curr_answer[m-first, 12])\n",
        "                else:\n",
        "                    curr_answer[m-last+1, 2] = max(score, curr_answer[m-last+1, 2])\n",
        "                    curr_answer[m-first, 7] = max(score, curr_answer[m-first, 7])\n",
        "                suffix_starts.add(m-last)\n",
        "            for start in root_starts:\n",
        "                root_ends = self.left_morphemes_[\"root\"].descend_by_prefixes(word[start:])\n",
        "                for end in root_ends:\n",
        "                    score = self._get_ngram_score(word[start:end], \"root\")\n",
        "                    if end == start+1:\n",
        "                        curr_answer[start + 1, 11] = max(score, curr_answer[start + 1, 11])\n",
        "                    else:\n",
        "                        curr_answer[start + 1, 1] = max(score, curr_answer[start + 1, 1])\n",
        "                        curr_answer[end, 6] = max(score, curr_answer[end, 6])\n",
        "            for end in suffix_starts:\n",
        "                root_lengths = self.right_morphemes_[\"root\"].descend_by_prefixes(word[end-1:-1:-1])\n",
        "                for k in root_lengths:\n",
        "                    score = self._get_ngram_score(word[end-k:end], 'root')\n",
        "                    if k == 1:\n",
        "                        curr_answer[end, 11] = max(curr_answer[end, 11], score)\n",
        "                    else:\n",
        "                        curr_answer[end-k+1, 1] = max(curr_answer[end-k+1, 1], score)\n",
        "                        curr_answer[end, 6] = max(curr_answer[end, 6], score)\n",
        "            answer[j] = curr_answer\n",
        "        return answer\n",
        "\n",
        "    def _make_morpheme_data_simple(self, data, bucket_length):\n",
        "        answer = np.zeros(shape=(len(data), bucket_length, 3), dtype=float)\n",
        "        for j, word in enumerate(data):\n",
        "            m = len(word)\n",
        "            curr_answer = np.zeros(shape=(bucket_length, 3), dtype=int)\n",
        "            positions = self.morpheme_trie_.find_substrings(word, return_positions=True)\n",
        "            for starts, end in positions:\n",
        "                for start in starts:\n",
        "                    score = self._get_ngram_score(word[start:end])\n",
        "                    if end == start+1:\n",
        "                        curr_answer[start+1, 2] = max(curr_answer[start+1, 2], score)\n",
        "                    else:\n",
        "                        curr_answer[start+1, 0] = max(curr_answer[start+0, 2], score)\n",
        "                        curr_answer[end, 1] = max(curr_answer[end, 1], score)\n",
        "            answer[j] = curr_answer\n",
        "        return answer\n",
        "\n",
        "    def _get_ngram_score(self, ngram, mode=\"None\"):\n",
        "        if self.to_memorize_ngram_counts:\n",
        "            return self.morpheme_counts_[mode].get(ngram, 0)\n",
        "        else:\n",
        "            return 1.0\n",
        "\n",
        "    def train(self, source, targets, dev=None, dev_targets=None, model_file=None):\n",
        "        \"\"\"\n",
        "        source: list of strs, список слов для морфемоделения\n",
        "        targets: list of strs, метки морфемоделения в формате BMES\n",
        "        model_file: str or None, default=None, файл для сохранения моделей\n",
        "        Возвращает:\n",
        "        -------------\n",
        "        self, обученный морфемоделитель\n",
        "        \"\"\"\n",
        "        self.symbols_, self.symbol_codes_ = _make_vocabulary(source)\n",
        "        self.target_symbols_, self.target_symbol_codes_ = _make_vocabulary(targets)\n",
        "        if self.to_memorize_morphemes:\n",
        "            self._memorize_morphemes(source, targets)\n",
        "\n",
        "        data_by_buckets, targets_by_buckets, _ = self._preprocess(source, targets)\n",
        "        if dev is not None:\n",
        "            dev_data_by_buckets, dev_targets_by_buckets, _ = self._preprocess(dev, dev_targets)\n",
        "        else:\n",
        "            dev_data_by_buckets, dev_targets_by_buckets = None, None\n",
        "        self.build()\n",
        "        self._train_models(data_by_buckets, targets_by_buckets,  dev_data_by_buckets,\n",
        "                           dev_targets_by_buckets, model_file=model_file)\n",
        "        return self\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\"\n",
        "        Создаёт нейронные модели\n",
        "        \"\"\"\n",
        "        self.models_ = [self.build_model() for _ in range(self.models_number)]\n",
        "        print(self.models_[0].summary())\n",
        "        return self\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Функция, задающая архитектуру нейронной сети\n",
        "        \"\"\"\n",
        "        # symbol_inputs: array, 1D-массив длины m\n",
        "        symbol_inputs = kl.Input(shape=(None,), dtype='uint8', name=\"symbol_inputs\")\n",
        "        # symbol_embeddings: array, 2D-массив размера m*self.symbols_number\n",
        "        if self.use_embeddings:\n",
        "            symbol_embeddings = kl.Embedding(self.symbols_number_, self.embeddings_size,\n",
        "                                             name=\"symbol_embeddings\")(symbol_inputs)\n",
        "        else:\n",
        "            symbol_embeddings = kl.Lambda(kb.one_hot, output_shape=(None, self.symbols_number_),\n",
        "                                          arguments={\"num_classes\": self.symbols_number_},\n",
        "                                          name=\"symbol_embeddings\")(symbol_inputs)\n",
        "        inputs = [symbol_inputs]\n",
        "        if self.to_memorize_morphemes:\n",
        "            # context_inputs: array, 2D-массив размера m*15\n",
        "            context_inputs = kl.Input(shape=(None, self.memory_dim), dtype='float32', name=\"context_inputs\")\n",
        "            inputs.append(context_inputs)\n",
        "            if self.context_dropout > 0.0:\n",
        "                context_inputs = kl.Dropout(self.context_dropout)(context_inputs)\n",
        "            # представление контекста подклеивается к представлению символа\n",
        "            symbol_embeddings = kl.Concatenate()([symbol_embeddings, context_inputs])\n",
        "        conv_inputs = symbol_embeddings\n",
        "        conv_outputs = []\n",
        "        for window_size, curr_filters_numbers in zip(self.window_size, self.filters_number):\n",
        "            # свёрточный слой отдельно для каждой ширины окна\n",
        "            curr_conv_input = conv_inputs\n",
        "            for j, filters_number in enumerate(curr_filters_numbers[:-1]):\n",
        "                # все слои свёртки, кроме финального (после них возможен dropout)\n",
        "                curr_conv_input = kl.Conv1D(filters_number, window_size,\n",
        "                                            activation=\"relu\", padding=\"same\")(curr_conv_input)\n",
        "                if self.dropout > 0.0:\n",
        "                    # между однотипными слоями рекомендуется вставить dropout\n",
        "                    curr_conv_input = kl.Dropout(self.dropout)(curr_conv_input)\n",
        "            if not self.use_lstm:\n",
        "                curr_conv_output = kl.Conv1D(curr_filters_numbers[-1], window_size,\n",
        "                                             activation=\"relu\", padding=\"same\")(curr_conv_input)\n",
        "            else:\n",
        "                curr_conv_output = curr_conv_input\n",
        "            conv_outputs.append(curr_conv_output)\n",
        "        # соединяем выходы всех свёрточных слоёв в один вектор\n",
        "        if len(conv_outputs) == 1:\n",
        "            conv_output = conv_outputs[0]\n",
        "        else:\n",
        "            conv_output = kl.Concatenate(name=\"conv_output\")(conv_outputs)\n",
        "        if self.use_lstm:\n",
        "            conv_output = kl.Bidirectional(\n",
        "                kl.LSTM(self.lstm_units, return_sequences=True))(conv_output)\n",
        "        if self.dense_output_units:\n",
        "            pre_last_output = kl.TimeDistributed(\n",
        "                kl.Dense(self.dense_output_units, activation=\"relu\"),\n",
        "                name=\"pre_output\")(conv_output)\n",
        "        else:\n",
        "            pre_last_output = conv_output\n",
        "        # финальный слой с softmax-активацией, чтобы получить распределение вероятностей\n",
        "        output = kl.TimeDistributed(\n",
        "            kl.Dense(self.target_symbols_number_, activation=\"softmax\"), name=\"output\")(pre_last_output)\n",
        "        model = Model(inputs, [output])\n",
        "        model.compile(optimizer=Adam(clipnorm=5.0),\n",
        "                      loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "    def _train_models(self, data_by_buckets, targets_by_buckets,\n",
        "                      dev_data_by_buckets=None, dev_targets_by_buckets=None, model_file=None):\n",
        "        \"\"\"\n",
        "        data_by_buckets: list of lists of np.arrays,\n",
        "            data_by_buckets[i] = [..., bucket_i, ...],\n",
        "            bucket = [input_1, ..., input_k],\n",
        "            input_j --- j-ый вход нейронной сети, вычисленный для текущей корзины\n",
        "        targets_by_buckets: list of np.arrays,\n",
        "            targets_by_buckets[i] --- закодированные ответы для i-ой корзины\n",
        "        model_file: str or None, путь к файлу для сохранения модели\n",
        "        \"\"\"\n",
        "        train_indexes_by_buckets, dev_indexes_by_buckets = [], []\n",
        "        if dev_data_by_buckets is not None:\n",
        "            train_indexes_by_buckets = [list(range(len(bucket[0]))) for bucket in data_by_buckets]\n",
        "            for elem in train_indexes_by_buckets:\n",
        "                np.random.shuffle(elem)\n",
        "            dev_indexes_by_buckets = [list(range(len(bucket[0]))) for bucket in dev_data_by_buckets]\n",
        "            train_data, dev_data = data_by_buckets, dev_data_by_buckets\n",
        "            train_targets, dev_targets = targets_by_buckets, dev_targets_by_buckets\n",
        "        else:\n",
        "            for bucket in data_by_buckets:\n",
        "                # разбиваем каждую корзину на обучающую и валидационную выборку\n",
        "                L = len(bucket[0])\n",
        "                indexes_for_bucket = list(range(L))\n",
        "                np.random.shuffle(indexes_for_bucket)\n",
        "                train_bucket_length = int(L*(1.0 - self.validation_split))\n",
        "                train_indexes_by_buckets.append(indexes_for_bucket[:train_bucket_length])\n",
        "                dev_indexes_by_buckets.append(indexes_for_bucket[train_bucket_length:])\n",
        "            train_data, dev_data = data_by_buckets, data_by_buckets\n",
        "            train_targets, dev_targets = targets_by_buckets, targets_by_buckets\n",
        "        # разбиваем на батчи обучающую и валидационную выборку\n",
        "        # (для валидационной этого можно не делать, а подавать сразу корзины)\n",
        "        train_batches_indexes = list(chain.from_iterable(\n",
        "            [[(i, elem[j:j+self.batch_size]) for j in range(0, len(elem), self.batch_size)]\n",
        "             for i, elem in enumerate(train_indexes_by_buckets)]))\n",
        "        dev_batches_indexes = list(chain.from_iterable(\n",
        "            [[(i, elem[j:j+self.batch_size]) for j in range(0, len(elem), self.batch_size)]\n",
        "             for i, elem in enumerate(dev_indexes_by_buckets)]))\n",
        "        # поскольку функции fit_generator нужен генератор, порождающий batch за batch'ем,\n",
        "        # то приходится заводить генераторы для обеих выборок\n",
        "        train_gen = generate_data(train_data, train_targets, train_batches_indexes,\n",
        "                                  classes_number=self.target_symbols_number_, shuffle=True)\n",
        "        val_gen = generate_data(dev_data, dev_targets, dev_batches_indexes,\n",
        "                                classes_number=self.target_symbols_number_, shuffle=False)\n",
        "        for i, model in enumerate(self.models_):\n",
        "            if model_file is not None:\n",
        "                curr_model_file = make_model_file(model_file, i+1)\n",
        "                # для сохранения модели с наилучшим результатом на валидационной выборке\n",
        "                save_callback = ModelCheckpoint(curr_model_file, save_weights_only=True, save_best_only=True)\n",
        "                curr_callbacks = self.callbacks + [save_callback]\n",
        "            else:\n",
        "                curr_callbacks = self.callbacks\n",
        "            model.fit_generator(train_gen, len(train_batches_indexes),\n",
        "                                epochs=self.nepochs, callbacks=curr_callbacks,\n",
        "                                validation_data=val_gen, validation_steps=len(dev_batches_indexes))\n",
        "            if model_file is not None:\n",
        "                model.load_weights(curr_model_file)\n",
        "        return self\n",
        "\n",
        "    def _memorize_morphemes(self, words, targets):\n",
        "        \"\"\"\n",
        "        запоминает морфемы. встречающиеся в словах обучающей выборки\n",
        "        \"\"\"\n",
        "        morphemes = defaultdict(lambda: defaultdict(int))\n",
        "        for word, target in zip(words, targets):\n",
        "            start = None\n",
        "            for i, (symbol, label) in enumerate(zip(word, target)):\n",
        "                if label.startswith(\"B-\"):\n",
        "                    start = i\n",
        "                elif label.startswith(\"E-\"):\n",
        "                    dest = extract_morpheme_type(label)\n",
        "                    morphemes[dest][word[start:i+1]] += 1\n",
        "                elif label.startswith(\"S-\"):\n",
        "                    dest = extract_morpheme_type(label)\n",
        "                    morphemes[dest][word[i]] += 1\n",
        "                elif label == END:\n",
        "                    break\n",
        "        self.morphemes_ = dict()\n",
        "        for key, counts in morphemes.items():\n",
        "            self.morphemes_[key] = [x for x, count in counts.items() if count >= self.min_morpheme_count]\n",
        "        self._make_morpheme_tries()\n",
        "        if self.to_memorize_ngram_counts:\n",
        "            self._memorize_ngram_counts(words, morphemes)\n",
        "        return self\n",
        "\n",
        "    def _memorize_ngram_counts(self, words, counts):\n",
        "        \"\"\"\n",
        "        запоминает частоты морфем, встречающихся в словах обучающей выборки\n",
        "        \"\"\"\n",
        "        prefix_counts, suffix_counts, ngram_counts  = defaultdict(int), defaultdict(int), defaultdict(int)\n",
        "        for i, word in enumerate(words, 1):\n",
        "            if i % 5000 == 0:\n",
        "                print(\"{} words processed\".format(i))\n",
        "            positions = self.morpheme_trie_.find_substrings(word, return_positions=True)\n",
        "            for starts, end in positions:\n",
        "                for start in starts:\n",
        "                    segment = word[start:end]\n",
        "                    ngram_counts[segment] += 1\n",
        "                    if start == 0:\n",
        "                        prefix_counts[segment] += 1\n",
        "                    if end == len(word):\n",
        "                        suffix_counts[segment] += 1\n",
        "        self.morpheme_counts_ = dict()\n",
        "        for key, curr_counts in counts.items():\n",
        "            curr_relative_counts = dict()\n",
        "            curr_ngram_counts = (prefix_counts if key == \"pref\" else\n",
        "                                 suffix_counts if key in [\"end\", \"postfix\"] else ngram_counts)\n",
        "            for ngram, count in curr_counts.items():\n",
        "                if count < self.min_morpheme_count or ngram not in curr_ngram_counts:\n",
        "                    continue\n",
        "                relative_count = min(count / curr_ngram_counts[ngram], 1.0)\n",
        "                if relative_count >= self.min_relative_ngram_count:\n",
        "                    curr_relative_counts[ngram] = relative_count\n",
        "            self.morpheme_counts_[key] = curr_relative_counts\n",
        "        return self\n",
        "\n",
        "    def _make_morpheme_tries(self):\n",
        "        \"\"\"\n",
        "        строит префиксный бор для морфем для более быстрого их поиска\n",
        "        \"\"\"\n",
        "        self.left_morphemes_, self.right_morphemes_ = dict(), dict()\n",
        "        if self.use_morpheme_types:\n",
        "            for key in self.LEFT_MORPHEME_TYPES:\n",
        "                self.left_morphemes_[key] = make_trie(list(self.morphemes_[key]))\n",
        "            for key in self.RIGHT_MORPHEME_TYPES:\n",
        "                self.right_morphemes_[key] = make_trie([x[::-1] for x in self.morphemes_[key]])\n",
        "        if not self.use_morpheme_types or self.to_memorize_ngram_counts:\n",
        "            morphemes = {x for elem in self.morphemes_.values() for x in elem}\n",
        "            self.morpheme_trie_ = make_trie(list(morphemes))\n",
        "        return self\n",
        "\n",
        "    def _predict_probs(self, words):\n",
        "        \"\"\"\n",
        "        data = [word_1, ..., word_m]\n",
        "        Возвращает:\n",
        "        -------------\n",
        "        answer = [probs_1, ..., probs_m]\n",
        "        probs_i = [p_1, ..., p_k], k = len(word_i)\n",
        "        p_j = [p_j1, ..., p_jr], r --- число классов\n",
        "        (len(AUXILIARY) + 4 * 4 (BMES; PREF, ROOT, SUFF, END) + 3 (BME; POSTFIX) + 2 * 1 (S; LINK, HYPHEN) = 23)\n",
        "        \"\"\"\n",
        "        data_by_buckets, indexes_by_buckets = self._preprocess(words)\n",
        "        word_probs = [None] * len(words)\n",
        "        for r, (bucket_data, (_, bucket_indexes)) in\\\n",
        "                enumerate(zip(data_by_buckets, indexes_by_buckets), 1):\n",
        "            print(\"Bucket {} predicting\".format(r))\n",
        "            bucket_probs = np.mean([model.predict(bucket_data) for model in self.models_], axis=0)\n",
        "            for i, elem in zip(bucket_indexes, bucket_probs):\n",
        "                word_probs[i] = elem\n",
        "        answer = [None] * len(words)\n",
        "        for i, (elem, word) in enumerate(zip(word_probs, words)):\n",
        "            if i % 1000 == 0 and i > 0:\n",
        "                print(\"{} words decoded\".format(i))\n",
        "            answer[i] = self._decode_best(elem, len(word))\n",
        "        return answer\n",
        "\n",
        "    def labels_to_morphemes(self, word, labels, probs=None, return_probs=False, return_types=False):\n",
        "        \"\"\"\n",
        "        Преобразует ответ из формата BMES в список морфем\n",
        "        Дополнительно может возвращать список вероятностей морфем\n",
        "        word: str, текущее слово,\n",
        "        labels: list of strs, предсказанные метки в формате BMES,\n",
        "        probs: list of floats or None, предсказанные вероятности меток\n",
        "        answer = [morphemes, (morpheme_probs), (morpheme_types)]\n",
        "            morphemes: list of strs, список морфем\n",
        "            morpheme_probs: list of floats, список вероятностей морфем\n",
        "            morpheme_types: list of strs, список типов морфем\n",
        "        \"\"\"\n",
        "        morphemes, curr_morpheme, morpheme_types = [], \"\", []\n",
        "        if self.use_morpheme_types:\n",
        "            end_labels = ['E-ROOT', 'E-PREF', 'E-SUFF', 'E-END', 'E-POSTFIX', 'S-ROOT',\n",
        "                          'S-PREF', 'S-SUFF', 'S-END', 'S-LINK', 'S-HYPH']\n",
        "        else:\n",
        "            end_labels = ['E-None', 'S-None']\n",
        "        for letter, label in zip(word, labels):\n",
        "            curr_morpheme += letter\n",
        "            if label in end_labels:\n",
        "                morphemes.append(curr_morpheme)\n",
        "                curr_morpheme = \"\"\n",
        "                morpheme_types.append(label.split(\"-\")[-1])\n",
        "        if return_probs:\n",
        "            if probs is None:\n",
        "                Warning(\"Для вычисления вероятностей морфем нужно передать вероятности меток\")\n",
        "                return_probs = False\n",
        "        if return_probs:\n",
        "            morpheme_probs, curr_morpheme_prob = [], 1.0\n",
        "            for label, prob in zip(labels, probs):\n",
        "                curr_morpheme_prob *= prob[self.target_symbol_codes_[label]]\n",
        "                if label in end_labels:\n",
        "                    morpheme_probs.append(curr_morpheme_prob)\n",
        "                    curr_morpheme_prob = 1.0\n",
        "            answer = [morphemes, morpheme_probs]\n",
        "        else:\n",
        "            answer = [morphemes]\n",
        "        if return_types:\n",
        "            answer.append(morpheme_types)\n",
        "        return answer\n",
        "\n",
        "    def predict(self, words, return_probs=False):\n",
        "        labels_with_probs = self._predict_probs(words)\n",
        "        return [self.labels_to_morphemes(word, elem[0], elem[1], return_probs=return_probs)\n",
        "                for elem, word in zip(labels_with_probs, words)]\n",
        "\n",
        "    def _decode_best(self, probs, length):\n",
        "        \"\"\"\n",
        "        Поддерживаем в каждой позиции наилучшие гипотезы для каждого состояния\n",
        "        Состояние --- последняя предсказанняя метка\n",
        "        \"\"\"\n",
        "        # вначале нужно проверить заведомо наилучший вариант на корректность\n",
        "        best_states = np.argmax(probs[:1+length], axis=1)\n",
        "        best_labels = [self.target_symbols_[state_index] for state_index in best_states]\n",
        "        if not is_correct_morpheme_sequence(best_labels[1:]):\n",
        "            # наилучший вариант оказался некорректным\n",
        "            initial_costs = [np.inf] * self.target_symbols_number_\n",
        "            initial_states = [None] * self.target_symbols_number_\n",
        "            initial_costs[BEGIN], initial_states[BEGIN] = -np.log(probs[0, BEGIN]), BEGIN\n",
        "            costs, states = [initial_costs], [initial_states]\n",
        "            for i in range(length):\n",
        "                # состояний мало, поэтому можно сортировать на каждом шаге\n",
        "                state_order = np.argsort(costs[-1])\n",
        "                curr_costs = [np.inf] * self.target_symbols_number_\n",
        "                prev_states = [None] * self.target_symbols_number_\n",
        "                inf_count = self.target_symbols_number_\n",
        "                for prev_state in state_order:\n",
        "                    if np.isinf(costs[-1][prev_state]):\n",
        "                        break\n",
        "                    elif prev_state in AUXILIARY_CODES and i != 0:\n",
        "                        continue\n",
        "                    possible_states = self.get_possible_next_states(prev_state)\n",
        "                    for state in possible_states:\n",
        "                        if np.isinf(curr_costs[state]):\n",
        "                            # поскольку новая вероятность не зависит от state,\n",
        "                            # а старые перебираются по возрастанию штрафа,\n",
        "                            # то оптимальное значение будет при первом обновлении\n",
        "                            curr_costs[state] = costs[-1][prev_state] - np.log(probs[i+1,state])\n",
        "                            prev_states[state] = prev_state\n",
        "                            inf_count -= 1\n",
        "                    if inf_count == len(AUXILIARY_CODES):\n",
        "                        # все вероятности уже посчитаны\n",
        "                        break\n",
        "                costs.append(curr_costs)\n",
        "                states.append(prev_states)\n",
        "            # последнее состояние --- обязательно конец морфемы\n",
        "            possible_states = [self.target_symbol_codes_[\"{}-{}\".format(x, y)]\n",
        "                               for x in \"ES\" for y in [\"ROOT\", \"SUFF\", \"END\", \"POSTFIX\", \"None\"]\n",
        "                               if \"{}-{}\".format(x, y) in self.target_symbol_codes_]\n",
        "            best_states = [min(possible_states, key=(lambda x: costs[-1][x]))]\n",
        "            for j in range(length, 0, -1):\n",
        "                # предыдущее состояние\n",
        "                best_states.append(states[j][best_states[-1]])\n",
        "            best_states = best_states[::-1]\n",
        "        probs_to_return = np.zeros(shape=(length, self.target_symbols_number_), dtype=np.float32)\n",
        "        # убираем невозможные состояния\n",
        "        for j, state in enumerate(best_states[:-1]):\n",
        "            possible_states = self.get_possible_next_states(state)\n",
        "            # оставляем только возможные состояния.\n",
        "            probs_to_return[j,possible_states] = probs[j+1,possible_states]\n",
        "        return [self.target_symbols_[i] for i in best_states[1:]], probs_to_return\n",
        "\n",
        "    def get_possible_next_states(self, state_index):\n",
        "        state = self.target_symbols_[state_index]\n",
        "        next_states = get_next_morpheme(state)\n",
        "        return [self.target_symbol_codes_[x] for x in next_states if x in self.target_symbol_codes_]\n"
      ],
      "metadata": {
        "id": "lhHzqtjdpPDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(data, targets, indexes, classes_number, shuffle=False, nepochs=None):\n",
        "    nsteps = 0\n",
        "    while nepochs is None or nsteps < nepochs:\n",
        "        if shuffle:\n",
        "            np.random.shuffle(indexes)\n",
        "        for i, bucket_indexes in indexes:\n",
        "            curr_bucket, curr_targets = data[i], targets[i]\n",
        "            data_to_yield = [elem[bucket_indexes] for elem in curr_bucket]\n",
        "            targets_to_yield = to_one_hot(curr_targets[bucket_indexes], classes_number)\n",
        "            yield data_to_yield, targets_to_yield\n",
        "        nsteps += 1\n"
      ],
      "metadata": {
        "id": "QAGGQnDAvC4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_quality(targets, predicted_targets, english_metrics=False, measure_last=True):\n",
        "\n",
        "    TP, FP, FN, equal, total = 0, 0, 0, 0, 0\n",
        "    SE = ['{}-{}'.format(x, y) for x in \"SE\" for y in [\"ROOT\", \"PREF\", \"SUFF\", \"END\", \"LINK\", \"None\"]]\n",
        "    # SE = ['S-ROOT', 'S-PREF', 'S-SUFF', 'S-END', 'S-LINK', 'E-ROOT', 'E-PREF', 'E-SUFF', 'E-END']\n",
        "    corr_words = 0\n",
        "    for corr, pred in zip(targets, predicted_targets):\n",
        "        corr_len = len(corr) + int(measure_last) - 1\n",
        "        pred_len = len(pred) + int(measure_last) - 1\n",
        "        boundaries = [i for i in range(corr_len) if corr[i] in SE]\n",
        "        pred_boundaries = [i for i in range(pred_len) if pred[i] in SE]\n",
        "        common = [x for x in boundaries if x in pred_boundaries]\n",
        "        TP += len(common)\n",
        "        FN += len(boundaries) - len(common)\n",
        "        FP += len(pred_boundaries) - len(common)\n",
        "        equal += sum(int(x==y) for x, y in zip(corr, pred))\n",
        "        total += len(corr)\n",
        "        corr_words += (corr == pred)\n",
        "    metrics = [\"Точность\", \"Полнота\", \"F1-мера\", \"Корректность\", \"Точность по словам\"]\n",
        "    if english_metrics:\n",
        "        metrics = [\"Precision\", \"Recall\", \"F1\", \"Accuracy\", \"Word accuracy\"]\n",
        "    results = [TP / (TP+FP), TP / (TP+FN), TP / (TP + 0.5*(FP+FN)),\n",
        "               equal / total, corr_words / len(targets)]\n",
        "    answer = list(zip(metrics, results))\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "Qou-ZQUkvM-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SHORT_ARGS = \"a:\""
      ],
      "metadata": {
        "id": "3x8QdiA7vW8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('new_fin10.txt') as f:\n",
        "    fin = list(map(str.strip, f.readlines()))"
      ],
      "metadata": {
        "id": "VTEZ9UCdynCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "swXBrHmqyxW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns = ['original', 'markup'])\n",
        "\n",
        "for line in fin:\n",
        "  if line:\n",
        "    line = line.replace(' ', '\\t')\n",
        "    original, markup = line.split('\\t')\n",
        "    d = {'original': original, 'markup': markup}\n",
        "    df = df.append(d, ignore_index=True)"
      ],
      "metadata": {
        "id": "gHZc-mq9y17g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "#train1, test1 = train_test_split(df, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "sch7sHqVl_Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "h5qckwOCxPxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_csv('fin.train', index=False, header=False, sep='\\t')\n",
        "test.to_csv('fin.dev', index=False, header=False, sep='\\t')"
      ],
      "metadata": {
        "id": "4_Rft2GU1JMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "dic_t = {\n",
        "    \"train_file\": \"./fin.train\",\n",
        "    \"dev_file\": \"./fin.dev\",\n",
        "    \"use_morpheme_types\": False,\n",
        "    \"model_params\": {\"conv_layers\": 2, \"window_size\": [3, 5], \"filters_number\": 96,\n",
        "                     \"dense_output_units\": 64, \"nepochs\": 80, \"validation_split\": 0.2,\n",
        "                     \"early_stopping\": 10, \"to_memorize_morphemes\": True, \"dropout\": 0.3,\n",
        "                     \"to_memorize_ngram_counts\": False, \"context_dropout\": 0.2, \"models_number\": 1},\n",
        "    \"save_file\": \"./morphemes-3-5-fin.json\", \"model_file\": \"./morphemes-model-3-5-fin.hdf5\",\n",
        "    \"test_file\": \"./fin.dev\", \"outfile\": \"./test_3-5-fin.txt\",\n",
        "    \"output_probs\": False, \"output_morpheme_types\": False\n",
        "}\n",
        "\n",
        "with open('config.json', 'w') as f:\n",
        "  json.dump(dic_t, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "9ZfK_RIol6zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(261) # для воспроизводимости\n",
        "    if len(sys.argv) < 2:\n",
        "        sys.exit(\"Pass config file\")\n",
        "    params = dic_t\n",
        "    use_morpheme_types = params[\"use_morpheme_types\"]\n",
        "    read_func = read_BMES if use_morpheme_types else read_splitted\n",
        "    if \"train_file\" in params:\n",
        "        n = params.get(\"n_train\") # число слов в обучающей+развивающей выборке\n",
        "        inputs, targets = read_func(params[\"train_file\"], n=n)\n",
        "        if \"dev_file\" in params:\n",
        "            n = params.get(\"n_dev\")  # число слов в обучающей+развивающей выборке\n",
        "            dev_inputs, dev_targets = read_func(params[\"dev_file\"], n=n)\n",
        "        else:\n",
        "            dev_inputs, dev_targets = None, None\n",
        "        # inputs, targets = read_input(params[\"train_file\"], n=n)\n",
        "    else:\n",
        "        inputs, targets, dev_inputs, dev_targets = None, None, None, None\n",
        "    if not \"load_file\" in params:\n",
        "        partitioner_params = params.get(\"model_params\", dict())\n",
        "        partitioner_params[\"use_morpheme_types\"] = use_morpheme_types\n",
        "        cls = Partitioner(**partitioner_params)\n",
        "    else:\n",
        "        cls = load_cls(params[\"load_file\"])\n",
        "    if inputs is not None:\n",
        "        cls.train(inputs, targets, dev_inputs, dev_targets, model_file=params.get(\"model_file\"))\n",
        "    if \"save_file\" in params:\n",
        "        model_file = params.get(\"model_file\")\n",
        "        cls.to_json(params[\"save_file\"], model_file)\n",
        "    if \"test_file\" in params:\n",
        "        inputs, targets = read_func(params[\"test_file\"], shuffle=False)\n",
        "        # inputs, targets = read_input(params[\"test_file\"])\n",
        "        predicted_targets = cls._predict_probs(inputs)\n",
        "        measure_last = params.get(\"measure_last\", use_morpheme_types)\n",
        "        quality = measure_quality(targets, [elem[0] for elem in predicted_targets],\n",
        "                                  english_metrics=params.get(\"english_metrics\", False),\n",
        "                                  measure_last=measure_last)\n",
        "        for key, value in sorted(quality):\n",
        "            print(\"{}={:.2f}\".format(key, 100*value))\n",
        "        if \"outfile\" in params:\n",
        "            outfile = params[\"outfile\"]\n",
        "            output_probs = params.get(\"output_probs\", True)\n",
        "            format_string = \"{}\\t{}\\t{}\\n\" if output_probs else \"{}\\t{}\\n\"\n",
        "            output_morpheme_types = params.get(\"output_morpheme_types\", True)\n",
        "            morph_format_string = \"{}\\t{}\" if output_morpheme_types else \"{}\"\n",
        "            with open(outfile, \"w\", encoding=\"utf8\") as fout:\n",
        "                for word, (labels, probs) in zip(inputs, predicted_targets):\n",
        "                    morphemes, morpheme_probs, morpheme_types = cls.labels_to_morphemes(\n",
        "                        word, labels, probs, return_probs=True, return_types=True)\n",
        "                    fout.write(format_string.format(\n",
        "                        word, \"/\".join(morph_format_string.format(*elem)\n",
        "                                       for elem in zip(morphemes, morpheme_types)),\n",
        "                        \" \".join(\"{:.2f}\".format(100*x) for x in morpheme_probs)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZGrxa9IvdID",
        "outputId": "074c7c43-4a0c-4979-fa34-65fbc97bc60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51\n",
            "25\n",
            "Processing morphemes for bucket length 9\n",
            "Processing morphemes for bucket length 9 finished\n",
            "Processing morphemes for bucket length 10\n",
            "Processing morphemes for bucket length 10 finished\n",
            "Processing morphemes for bucket length 11\n",
            "Processing morphemes for bucket length 11 finished\n",
            "Processing morphemes for bucket length 12\n",
            "Processing morphemes for bucket length 12 finished\n",
            "Processing morphemes for bucket length 13\n",
            "Processing morphemes for bucket length 13 finished\n",
            "Processing morphemes for bucket length 14\n",
            "Processing morphemes for bucket length 14 finished\n",
            "Processing morphemes for bucket length 16\n",
            "Processing morphemes for bucket length 16 finished\n",
            "Processing morphemes for bucket length 18\n",
            "Processing morphemes for bucket length 18 finished\n",
            "Processing morphemes for bucket length 23\n",
            "Processing morphemes for bucket length 23 finished\n",
            "Processing morphemes for bucket length 9\n",
            "Processing morphemes for bucket length 9 finished\n",
            "Processing morphemes for bucket length 11\n",
            "Processing morphemes for bucket length 11 finished\n",
            "Processing morphemes for bucket length 12\n",
            "Processing morphemes for bucket length 12 finished\n",
            "Processing morphemes for bucket length 13\n",
            "Processing morphemes for bucket length 13 finished\n",
            "Processing morphemes for bucket length 14\n",
            "Processing morphemes for bucket length 14 finished\n",
            "Processing morphemes for bucket length 16\n",
            "Processing morphemes for bucket length 16 finished\n",
            "Processing morphemes for bucket length 17\n",
            "Processing morphemes for bucket length 17 finished\n",
            "Processing morphemes for bucket length 21\n",
            "Processing morphemes for bucket length 21 finished\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " symbol_inputs (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " context_inputs (InputLayer)    [(None, None, 3)]    0           []                               \n",
            "                                                                                                  \n",
            " symbol_embeddings (Lambda)     (None, None, 30)     0           ['symbol_inputs[0][0]']          \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, None, 3)      0           ['context_inputs[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, None, 33)     0           ['symbol_embeddings[0][0]',      \n",
            "                                                                  'dropout[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, None, 96)     9600        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, None, 96)     15936       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, None, 96)     0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, None, 96)     0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, None, 96)     27744       ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, None, 96)     46176       ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " conv_output (Concatenate)      (None, None, 192)    0           ['conv1d_1[0][0]',               \n",
            "                                                                  'conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " pre_output (TimeDistributed)   (None, None, 64)     12352       ['conv_output[0][0]']            \n",
            "                                                                                                  \n",
            " output (TimeDistributed)       (None, None, 8)      520         ['pre_output[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 112,328\n",
            "Trainable params: 112,328\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:397: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.8577 - accuracy: 0.3633WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 4s 132ms/step - loss: 1.8577 - accuracy: 0.3633 - val_loss: 1.6189 - val_accuracy: 0.4163\n",
            "Epoch 2/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.5145 - accuracy: 0.4314WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 48ms/step - loss: 1.4902 - accuracy: 0.4214 - val_loss: 1.3477 - val_accuracy: 0.4163\n",
            "Epoch 3/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.1964 - accuracy: 0.4678WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 37ms/step - loss: 1.1815 - accuracy: 0.4642 - val_loss: 1.0312 - val_accuracy: 0.5422\n",
            "Epoch 4/80\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.8623 - accuracy: 0.6577WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 51ms/step - loss: 0.8604 - accuracy: 0.6581 - val_loss: 0.7085 - val_accuracy: 0.7654\n",
            "Epoch 5/80\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5502 - accuracy: 0.8366WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 0.5758 - accuracy: 0.8086 - val_loss: 0.5349 - val_accuracy: 0.8112\n",
            "Epoch 6/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.4720 - accuracy: 0.8238WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 0.4314 - accuracy: 0.8446 - val_loss: 0.4504 - val_accuracy: 0.8484\n",
            "Epoch 7/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3753 - accuracy: 0.8760WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.3572 - accuracy: 0.8771 - val_loss: 0.4504 - val_accuracy: 0.8498\n",
            "Epoch 8/80\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3681 - accuracy: 0.8621WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.3693 - accuracy: 0.8624 - val_loss: 0.3898 - val_accuracy: 0.8727\n",
            "Epoch 9/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3216 - accuracy: 0.8709WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 0.3213 - accuracy: 0.8709 - val_loss: 0.4202 - val_accuracy: 0.8469\n",
            "Epoch 10/80\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3174 - accuracy: 0.8765WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.3174 - accuracy: 0.8765 - val_loss: 0.4268 - val_accuracy: 0.8526\n",
            "Epoch 11/80\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.9009WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 0.2730 - accuracy: 0.9009 - val_loss: 0.3655 - val_accuracy: 0.8670\n",
            "Epoch 12/80\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2355 - accuracy: 0.9093WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 50ms/step - loss: 0.2391 - accuracy: 0.9070 - val_loss: 0.4137 - val_accuracy: 0.8526\n",
            "Epoch 13/80\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2625 - accuracy: 0.9094WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 54ms/step - loss: 0.2513 - accuracy: 0.9113 - val_loss: 0.3535 - val_accuracy: 0.8712\n",
            "Epoch 14/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2021 - accuracy: 0.9270WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.2193 - accuracy: 0.9168 - val_loss: 0.3541 - val_accuracy: 0.8670\n",
            "Epoch 15/80\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2006 - accuracy: 0.9244WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.2144 - accuracy: 0.9199 - val_loss: 0.3438 - val_accuracy: 0.8755\n",
            "Epoch 16/80\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2103 - accuracy: 0.9202WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.1766 - accuracy: 0.9376 - val_loss: 0.3481 - val_accuracy: 0.8698\n",
            "Epoch 17/80\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1810 - accuracy: 0.9321WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.1810 - accuracy: 0.9321 - val_loss: 0.3520 - val_accuracy: 0.8784\n",
            "Epoch 18/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1580 - accuracy: 0.9427WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.1543 - accuracy: 0.9443 - val_loss: 0.3516 - val_accuracy: 0.8670\n",
            "Epoch 19/80\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1258 - accuracy: 0.9603WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.1370 - accuracy: 0.9517 - val_loss: 0.3727 - val_accuracy: 0.8670\n",
            "Epoch 20/80\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1322 - accuracy: 0.9566WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.1447 - accuracy: 0.9486 - val_loss: 0.3577 - val_accuracy: 0.8712\n",
            "Epoch 21/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1185 - accuracy: 0.9530WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.1144 - accuracy: 0.9560 - val_loss: 0.3620 - val_accuracy: 0.8741\n",
            "Epoch 22/80\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1114 - accuracy: 0.9605WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.1099 - accuracy: 0.9621 - val_loss: 0.3700 - val_accuracy: 0.8698\n",
            "Epoch 23/80\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1015 - accuracy: 0.9674WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.0956 - accuracy: 0.9694 - val_loss: 0.4058 - val_accuracy: 0.8670\n",
            "Epoch 24/80\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1136 - accuracy: 0.9551WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 35ms/step - loss: 0.1099 - accuracy: 0.9572 - val_loss: 0.3686 - val_accuracy: 0.8712\n",
            "Epoch 25/80\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9719WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 51ms/step - loss: 0.0937 - accuracy: 0.9719 - val_loss: 0.3869 - val_accuracy: 0.8698\n",
            "Epoch 26/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.0695 - accuracy: 0.9825WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 0.0758 - accuracy: 0.9786 - val_loss: 0.4137 - val_accuracy: 0.8712\n",
            "Epoch 27/80\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9688WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.0805 - accuracy: 0.9688 - val_loss: 0.3915 - val_accuracy: 0.8684\n",
            "Epoch 28/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.0740 - accuracy: 0.9736WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.0694 - accuracy: 0.9743 - val_loss: 0.4090 - val_accuracy: 0.8727\n",
            "Epoch 29/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.0741 - accuracy: 0.9734WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0721 - accuracy: 0.9743 - val_loss: 0.4196 - val_accuracy: 0.8627\n",
            "Epoch 30/80\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.0724 - accuracy: 0.9728WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0628 - accuracy: 0.9749 - val_loss: 0.4121 - val_accuracy: 0.8670\n",
            "Epoch 31/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.0500 - accuracy: 0.9839WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0561 - accuracy: 0.9810 - val_loss: 0.4201 - val_accuracy: 0.8712\n",
            "Epoch 32/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0437 - accuracy: 0.9837WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0475 - accuracy: 0.9817 - val_loss: 0.4283 - val_accuracy: 0.8784\n",
            "Epoch 33/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0618 - accuracy: 0.9767WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0590 - accuracy: 0.9786 - val_loss: 0.4443 - val_accuracy: 0.8712\n",
            "Epoch 34/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0582 - accuracy: 0.9768WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0530 - accuracy: 0.9817 - val_loss: 0.4549 - val_accuracy: 0.8598\n",
            "Epoch 35/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0485 - accuracy: 0.9829WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0508 - accuracy: 0.9810 - val_loss: 0.4720 - val_accuracy: 0.8684\n",
            "Epoch 36/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0440 - accuracy: 0.9855WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0397 - accuracy: 0.9890 - val_loss: 0.4479 - val_accuracy: 0.8684\n",
            "Epoch 37/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0399 - accuracy: 0.9872WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0408 - accuracy: 0.9890 - val_loss: 0.4720 - val_accuracy: 0.8712\n",
            "Epoch 38/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0383 - accuracy: 0.9872WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0358 - accuracy: 0.9884 - val_loss: 0.4705 - val_accuracy: 0.8655\n",
            "Epoch 39/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0379 - accuracy: 0.9883WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0332 - accuracy: 0.9902 - val_loss: 0.4843 - val_accuracy: 0.8655\n",
            "Epoch 40/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0304 - accuracy: 0.9944WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0298 - accuracy: 0.9945 - val_loss: 0.4885 - val_accuracy: 0.8612\n",
            "Epoch 41/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.0305 - accuracy: 0.9899WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0316 - accuracy: 0.9890 - val_loss: 0.4968 - val_accuracy: 0.8627\n",
            "Epoch 42/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.0230 - accuracy: 0.9952WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0228 - accuracy: 0.9963 - val_loss: 0.4920 - val_accuracy: 0.8698\n",
            "Epoch 43/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.0276 - accuracy: 0.9898WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0294 - accuracy: 0.9890 - val_loss: 0.5091 - val_accuracy: 0.8741\n",
            "Epoch 44/80\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9939WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.0306 - accuracy: 0.9939 - val_loss: 0.5023 - val_accuracy: 0.8698\n",
            "Epoch 45/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0255 - accuracy: 0.9914WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0221 - accuracy: 0.9933 - val_loss: 0.5175 - val_accuracy: 0.8770\n",
            "Epoch 46/80\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.0321 - accuracy: 0.9853WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0258 - accuracy: 0.9896 - val_loss: 0.5071 - val_accuracy: 0.8727\n",
            "Epoch 47/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0187 - accuracy: 0.9936WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0183 - accuracy: 0.9939 - val_loss: 0.5140 - val_accuracy: 0.8712\n",
            "Epoch 48/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0182 - accuracy: 0.9983WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0203 - accuracy: 0.9963 - val_loss: 0.5269 - val_accuracy: 0.8684\n",
            "Epoch 49/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0210 - accuracy: 0.9909WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0253 - accuracy: 0.9896 - val_loss: 0.5191 - val_accuracy: 0.8741\n",
            "Epoch 50/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0135 - accuracy: 0.9970WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0174 - accuracy: 0.9945 - val_loss: 0.5116 - val_accuracy: 0.8798\n",
            "Epoch 51/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0230 - accuracy: 0.9900WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0209 - accuracy: 0.9920 - val_loss: 0.5503 - val_accuracy: 0.8684\n",
            "Epoch 52/80\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.0177 - accuracy: 0.9947WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0199 - accuracy: 0.9920 - val_loss: 0.5352 - val_accuracy: 0.8698\n",
            "Epoch 53/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0169 - accuracy: 0.9983WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0147 - accuracy: 0.9982 - val_loss: 0.5444 - val_accuracy: 0.8698\n",
            "Epoch 54/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0138 - accuracy: 0.9964WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0163 - accuracy: 0.9951 - val_loss: 0.5402 - val_accuracy: 0.8741\n",
            "Epoch 55/80\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.0316 - accuracy: 0.9942WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.0279 - accuracy: 0.9933 - val_loss: 0.5609 - val_accuracy: 0.8698\n",
            "Epoch 56/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.0189 - accuracy: 0.9953WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0176 - accuracy: 0.9957 - val_loss: 0.5715 - val_accuracy: 0.8698\n",
            "Epoch 57/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0132 - accuracy: 0.9966WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0125 - accuracy: 0.9963 - val_loss: 0.5681 - val_accuracy: 0.8670\n",
            "Epoch 58/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0196 - accuracy: 0.9966WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0198 - accuracy: 0.9951 - val_loss: 0.5669 - val_accuracy: 0.8670\n",
            "Epoch 59/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0077 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0125 - accuracy: 0.9957 - val_loss: 0.5785 - val_accuracy: 0.8670\n",
            "Epoch 60/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0109 - accuracy: 0.9974WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.5522 - val_accuracy: 0.8698\n",
            "Epoch 61/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0077 - accuracy: 0.9991WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0108 - accuracy: 0.9982 - val_loss: 0.5804 - val_accuracy: 0.8741\n",
            "Epoch 62/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0078 - accuracy: 0.9990WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.5707 - val_accuracy: 0.8698\n",
            "Epoch 63/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0143 - accuracy: 0.9947WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0120 - accuracy: 0.9963 - val_loss: 0.5732 - val_accuracy: 0.8712\n",
            "Epoch 64/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0081 - accuracy: 0.9981WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0075 - accuracy: 0.9988 - val_loss: 0.5812 - val_accuracy: 0.8798\n",
            "Epoch 65/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0103 - accuracy: 0.9972WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.0098 - accuracy: 0.9976 - val_loss: 0.6063 - val_accuracy: 0.8755\n",
            "Epoch 66/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0146 - accuracy: 0.9972WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0162 - accuracy: 0.9963 - val_loss: 0.5904 - val_accuracy: 0.8712\n",
            "Epoch 67/80\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.0136 - accuracy: 0.9939WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0117 - accuracy: 0.9957 - val_loss: 0.6172 - val_accuracy: 0.8627\n",
            "Epoch 68/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.6234 - val_accuracy: 0.8655\n",
            "Epoch 69/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.0148 - accuracy: 0.9966WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0139 - accuracy: 0.9963 - val_loss: 0.6000 - val_accuracy: 0.8741\n",
            "Epoch 70/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0087 - accuracy: 0.9973WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.6254 - val_accuracy: 0.8670\n",
            "Epoch 71/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.0056 - accuracy: 0.9992WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0082 - accuracy: 0.9988 - val_loss: 0.6074 - val_accuracy: 0.8641\n",
            "Epoch 72/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0090 - accuracy: 0.9991WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0074 - accuracy: 0.9994 - val_loss: 0.6059 - val_accuracy: 0.8684\n",
            "Epoch 73/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0074 - accuracy: 0.9969WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.0080 - accuracy: 0.9969 - val_loss: 0.5950 - val_accuracy: 0.8727\n",
            "Epoch 74/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0119 - accuracy: 0.9963WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0100 - accuracy: 0.9976 - val_loss: 0.6235 - val_accuracy: 0.8770\n",
            "Epoch 75/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0067 - accuracy: 0.9991WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.0082 - accuracy: 0.9988 - val_loss: 0.6081 - val_accuracy: 0.8727\n",
            "Epoch 76/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.6342 - val_accuracy: 0.8727\n",
            "Epoch 77/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.0099 - accuracy: 0.9983WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0098 - accuracy: 0.9976 - val_loss: 0.6422 - val_accuracy: 0.8784\n",
            "Epoch 78/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0087 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.6410 - val_accuracy: 0.8684\n",
            "Epoch 79/80\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.0056 - accuracy: 0.9991WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.0051 - accuracy: 0.9994 - val_loss: 0.6482 - val_accuracy: 0.8727\n",
            "Epoch 80/80\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0044 - accuracy: 0.9994 - val_loss: 0.6439 - val_accuracy: 0.8698\n",
            "Processing morphemes for bucket length 9\n",
            "Processing morphemes for bucket length 9 finished\n",
            "Processing morphemes for bucket length 11\n",
            "Processing morphemes for bucket length 11 finished\n",
            "Processing morphemes for bucket length 12\n",
            "Processing morphemes for bucket length 12 finished\n",
            "Processing morphemes for bucket length 13\n",
            "Processing morphemes for bucket length 13 finished\n",
            "Processing morphemes for bucket length 14\n",
            "Processing morphemes for bucket length 14 finished\n",
            "Processing morphemes for bucket length 16\n",
            "Processing morphemes for bucket length 16 finished\n",
            "Processing morphemes for bucket length 17\n",
            "Processing morphemes for bucket length 17 finished\n",
            "Processing morphemes for bucket length 21\n",
            "Processing morphemes for bucket length 21 finished\n",
            "Bucket 1 predicting\n",
            "Bucket 2 predicting\n",
            "Bucket 3 predicting\n",
            "Bucket 4 predicting\n",
            "Bucket 5 predicting\n",
            "Bucket 6 predicting\n",
            "Bucket 7 predicting\n",
            "Bucket 8 predicting\n",
            "F1-мера=80.83\n",
            "Корректность=86.07\n",
            "Полнота=75.19\n",
            "Точность=87.39\n",
            "Точность по словам=48.08\n"
          ]
        }
      ]
    }
  ]
}